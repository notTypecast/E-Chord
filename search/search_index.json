{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"E-Chord E-Chord is a Python implementation of the Chord protocol, a peer-to-peer lookup service. Tutorials and Guides Throughout this documentation, you will find tutorials and guides about various topics related to E-Chord. Take note that it is assumed that all execution is happening under a Linux environment, with Python 3 available. Features On top of the features of the base Chord protocol, the following ones have been added or updated: Seed Server : The new node learns the identity of an existing E-Chord node by contacting the Seed Server . Concurrent Joins : Multiple nodes joining concurrently is supported using a periodic stabilization routine. Handling Massive Node Failures : Network consistency is preserved after multiple nodes fail simultaneously. The stabilization routine is responsible for keeping each node's successor up to date. Data Storage : Each node can hold data in key-value pairs. Any node can be contacted to store such a pair, but the pair will be stored in a node determined by the network. As new nodes join the network, data is transferred to them accordingly. Load Balancing : Data is shared between multiple nodes, such that no one node holds a much larger amount of keys than others. In this way, the load is balanced between the nodes. Data Consistency : Data inserted into the network has a very high probability not to be lost, even if multiple nodes join or leave the network concurrently. Ring Split Prevention : By having each node keep a successor list of size log(n) , where n is the maximum number of nodes that can join the network, it is guaranteed that, in most situations, a massive node failure will not split the ring into multiple inconsistent parts. Custom Parameters : By editing the config/params.json file, various parameters regarding the network and scripts can be customized. It is required that all nodes run with the same parameters. Simulation Scripts : E-Chord can be simulated on a single machine using the provided simulation scripts. This helps visualize the network and test its capabilities. Potential Improvements Data Backups : If a node fails abruptly, all the data it contains will be lost. Data should be replicated across nodes to preserve consistency, even after massive node failures. References [1] Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. SIGCOMM Comput. Commun. Rev. 31, 4 (October 2001), 149\u2013160. DOI: https://doi.org/10.1145/964723.383071","title":"Home"},{"location":"#e-chord","text":"E-Chord is a Python implementation of the Chord protocol, a peer-to-peer lookup service.","title":"E-Chord"},{"location":"#tutorials-and-guides","text":"Throughout this documentation, you will find tutorials and guides about various topics related to E-Chord. Take note that it is assumed that all execution is happening under a Linux environment, with Python 3 available.","title":"Tutorials and Guides"},{"location":"#features","text":"On top of the features of the base Chord protocol, the following ones have been added or updated: Seed Server : The new node learns the identity of an existing E-Chord node by contacting the Seed Server . Concurrent Joins : Multiple nodes joining concurrently is supported using a periodic stabilization routine. Handling Massive Node Failures : Network consistency is preserved after multiple nodes fail simultaneously. The stabilization routine is responsible for keeping each node's successor up to date. Data Storage : Each node can hold data in key-value pairs. Any node can be contacted to store such a pair, but the pair will be stored in a node determined by the network. As new nodes join the network, data is transferred to them accordingly. Load Balancing : Data is shared between multiple nodes, such that no one node holds a much larger amount of keys than others. In this way, the load is balanced between the nodes. Data Consistency : Data inserted into the network has a very high probability not to be lost, even if multiple nodes join or leave the network concurrently. Ring Split Prevention : By having each node keep a successor list of size log(n) , where n is the maximum number of nodes that can join the network, it is guaranteed that, in most situations, a massive node failure will not split the ring into multiple inconsistent parts. Custom Parameters : By editing the config/params.json file, various parameters regarding the network and scripts can be customized. It is required that all nodes run with the same parameters. Simulation Scripts : E-Chord can be simulated on a single machine using the provided simulation scripts. This helps visualize the network and test its capabilities.","title":"Features"},{"location":"#potential-improvements","text":"Data Backups : If a node fails abruptly, all the data it contains will be lost. Data should be replicated across nodes to preserve consistency, even after massive node failures.","title":"Potential Improvements"},{"location":"#references","text":"[1] Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. SIGCOMM Comput. Commun. Rev. 31, 4 (October 2001), 149\u2013160. DOI: https://doi.org/10.1145/964723.383071","title":"References"},{"location":"discussion/architecture/","text":"E-Chord's architecture This discussion centers on the architecture of an E-Chord node. We'll cover aspects related to the way a node organizes its function and the rules behind its method of operation. Calling a remote function In distributed systems such as Chord, the need arises for executing certain procedures on different systems and obtaining their results. For instance, Chord's lookup procedure relies on a node's ability to get information about other nodes' pointers, such as successors and fingers. E-Chord solves this problem using RPC s (remote procedure calls). In general distributed computing terms, a remote procedure call occurs when software on a computer executes a procedure in an abstract way, such that the procedure could run on the computer itself, or on a different computer in the network. In E-Chord terms, any request made to a node triggers execution of an RPC. RPCs allow a node to execute the protocol's procedures on different nodes, thus retrieving information about parts of the network that the executing node might not have direct access to. Take a look at Chord's algorithm for locating the predecessor for an ID: Set the current node to this node. Check if the ID is contained between the current node and the current node's successor on the ring. If it is, return the current node. If it is not, find the node pointed to by the longest finger in the finger table of the current node, which appears to the left of our ID (counter-clockwise) on the ring. Set that node as the current node. Return to step 2. Notice how, throughout this algorithm's execution, it's possible (and likely) that the current node will be set to various different nodes that aren't the original node the algorithm runs on. The algorithm, however, requires checking whether the ID is between the current node and its successor. If it isn't, it requires checking the finger table of that node and locating the closest preceding finger to the ID. These steps make use of information contained in the routing tables of other nodes. Of course, it would be inefficient to pass all that information to the original node and process it there. Instead, we execute an RPC on the current node. That node could be the node itself, or some other node, but it will always return the results we need. Main architecture E-Chord nodes need to perform two types of tasks. Firstly, they need to be able to have RPCs executed on them. In other words, they must respond to requests. Secondly, they need to periodically execute stabilization routines, such that their routing information is kept up to date. To achieve this, E-Chord uses a threading approach. Specifically, each E-Chord node executes three basic threads. There is the main thread, the stabilizer thread and the listener thread. The listener thread Firstly, let's focus on the listener thread. The purpose of this thread is to host the server the node is running, listening for incoming requests. When this thread receives a request, it creates a new thread to handle that request. The reason we don't just handle the request in the listener itself becomes apparent when you consider a situation, in which many requests are received by a node at once. Imagine a node receiving many different requests at the same time. If we handled each of them individually in the listener thread, we'd have to handle them sequentially. Remember that handling a request might require making additional requests to other nodes, in a recursive manner. This is a problem, because what if the first request took a whole second to complete? That would mean the rest of the requests would need to wait in line before being handled. Instead, by starting a handler thread for each request, no request needs to wait for another to complete, in order to be handled. Requests that can be handled instantly (for example, a poll RPC) will be, whereas requests that might require more time (like a find_key RPC) won't block unrelated traffic. Shared resources This subsections describes the function of a mutex. You may skip to the next section if you are already familiar with it. Consider an integer with the value 0. Two threads both want to increment the value of that variable. Each thread runs the following code: x += 1 This might look like a single line of code, but internally, it looks more like this: READ x ADD 1 WRITE x As you can see, this isn't just one step anymore. Now imagine having two threads running this code. Internally, the CPU only runs one instruction at a time. As such, one possible execution flow is the following: Thread 1 Thread 2 READ x (x = 0) READ x (x =0) ADD 1 (x = 1) ADD 1 (x = 1) WRITE x (x = 1) WRITE x (x = 1) If Thread 1 had written the value (x = 1) before Thread 2 had read it, Thread 2 would have read 1 and correctly incremented it to 2. Yet, the final value for the variable in memory is 1. Since we run two threads, each incrementing the variable by 1, we expected it to be 2. We refer to x here as a shared resource. The solution to these types of problems is to use a mutex. A mutex is a special type of variable that can be locked and unlocked. Once a thread calls lock on the mutex, any other thread calling lock on it must wait until the first thread calls unlock in order to proceed. In the above example, if Thread 1 locks a mutex before entering, Thread 2 will be stuck waiting for Thread 1 to unlock that mutex. Thread 1 will do so after writing the value 1. Therefore, Thread 2 will correctly read 1 and increment it to 2. Routing information as a shared resource With the described system in place, we are able to respond to multiple requests in a non-blocking manner. However, this creates a different issue we must tackle. What happens when two requests both concern the same routing data in the node? For instance, what if two different requests are about the node's predecessor? In that case, that routing information becomes a shared resource. Two different requests (i.e two threads) need access to this resource at once. Such a situation needs to be handled with care. Consider one request that performs searching and another that updates routing information. If the latter performs the update while the former is executing, this could throw off the search algorithm and result in a routing failure. To combat this, E-Chord assigns an additional property to each of its threads. The main thread is considered a writer, while every other thread is considered a reader. These properties imply that only the main thread is able to alter any sort of data on the node. Other threads may only read that data, but never change it. To ensure this results in shared resources being handled correctly, we also employ the use of a Reader-Writer lock. This is a special type of mutex that allows any number of readers access to the shared resource at once, but only allows one writer access. On top of this, both the writer and readers cannot have simultaneous access. This works great, because it means that while a request is handled, no data on the node can be changed, since the thread handling it is a reader. Changes can only occur if the writer has access to the resource. In that case, we know none of the readers are executing, so we don't have to worry about routing issues related to the shared resources. The main thread Naturally, we expect that RPCs could change the data of a node. For instance, the update_predecessor RPC requires that a node update its predecessor node. Yet, the previous approach ensures that RPCs, which are always readers, can never change the node's data. How do we deal with this issue? As we saw, the main thread is considered the writer, so only it can make changes to data. Thus, any readers that want to make changes must somehow defer them, so they are made by the writer when it gets resource access. The method in which this happens utilizes the event queue. If a reader wants to make a change, it inserts a function that makes that change in the queue. Note that the function isn't executed, so the change is not made instantly. The main thread then retrieves data from that queue. If that data is a function, it means it is some change that a reader needed to make, so the main thread, being the writer, executes that function. Generally, the main thread repeatedly reads data from the queue. Once data is found, it requests access to the resource. When it gains access, it handles the data appropriately (for example, runs it if it is a function). Finally, it frees the resource so readers that are waiting can gain access to it, and repeats the process. The stabilizer thread The final thread of an E-Chord node is the stabilizer. This thread's function is simple. The thread waits for a certain amount of time, as described by the parameters, and then inserts a special token ( 0 ) into the queue. The stabilizer thread is also a reader and, thus, cannot run the stabilization procedures itself (since those require editing a node's data). By inserting the token, the main thread will know that stabilization needs to be run and can then run it itself. Connection pool By observing the Chord protocol, one might notice that there is a lot of network traffic used for stabilization. This ensures the network will always remain stable, but requires a large amount of bandwidth to achieve this. If we examine those requests further, we might notice that for a given network, a node will mostly only contact the same nodes periodically. If we were to open a new TCP socket every time we needed to make an RPC, there would be lots of unnecessary bandwidth overhead just for creating connections. E-Chord attempts to reduce that overhead by intelligently keeping connections alive, based on their rate of usage. Specifically, E-Chord uses a connection pool, whose purpose is to keep certain connections alive. This connection pool is a layer of abstraction. Whenever E-Chord makes a request, it defers that request to the connection pool. The connection pool determines whether an open connection exists with the target node and, if it does, uses that connection instead of opening a new one. Additionally, whenever stabilization routines are executed, we also execute cleanup routines for the connection pool, in order to close unused connections. The connection pool itself also handles issues related to synchronization. Since connections are kept open, multiple requests to the same node will use the same connection. Thus, open connections also become shared resources, so the connection pool handles this accordingly. Security If you're familiar with E-Chord's requests, you might notice that it would be very easy for a bad actor to disrupt the network. Any request received, if valid, is taken at face value by E-Chord nodes. In other words, it is assumed that all nodes in the network operate in an expected fashion. This is because E-Chord implements the overlay network itself, and doesn't concern itself with security. A second layer could theoretically be set up on top of it, which validates requests and ensures bad actors cannot disrupt network operation. Backups E-Chord also does not implement data backups. This means that any node that leaves abruptly causes all the data stored on it to be lost. When it comes to implementing backups, various methods could be used. One such idea would be to use multiple hash functions, thereby creating multiple virtual rings, and store each pair on every ring, instead of just once. Of course, this would increase network bandwidth significantly.","title":"E-Chord's architecture"},{"location":"discussion/architecture/#e-chords-architecture","text":"This discussion centers on the architecture of an E-Chord node. We'll cover aspects related to the way a node organizes its function and the rules behind its method of operation.","title":"E-Chord's architecture"},{"location":"discussion/architecture/#calling-a-remote-function","text":"In distributed systems such as Chord, the need arises for executing certain procedures on different systems and obtaining their results. For instance, Chord's lookup procedure relies on a node's ability to get information about other nodes' pointers, such as successors and fingers. E-Chord solves this problem using RPC s (remote procedure calls). In general distributed computing terms, a remote procedure call occurs when software on a computer executes a procedure in an abstract way, such that the procedure could run on the computer itself, or on a different computer in the network. In E-Chord terms, any request made to a node triggers execution of an RPC. RPCs allow a node to execute the protocol's procedures on different nodes, thus retrieving information about parts of the network that the executing node might not have direct access to. Take a look at Chord's algorithm for locating the predecessor for an ID: Set the current node to this node. Check if the ID is contained between the current node and the current node's successor on the ring. If it is, return the current node. If it is not, find the node pointed to by the longest finger in the finger table of the current node, which appears to the left of our ID (counter-clockwise) on the ring. Set that node as the current node. Return to step 2. Notice how, throughout this algorithm's execution, it's possible (and likely) that the current node will be set to various different nodes that aren't the original node the algorithm runs on. The algorithm, however, requires checking whether the ID is between the current node and its successor. If it isn't, it requires checking the finger table of that node and locating the closest preceding finger to the ID. These steps make use of information contained in the routing tables of other nodes. Of course, it would be inefficient to pass all that information to the original node and process it there. Instead, we execute an RPC on the current node. That node could be the node itself, or some other node, but it will always return the results we need.","title":"Calling a remote function"},{"location":"discussion/architecture/#main-architecture","text":"E-Chord nodes need to perform two types of tasks. Firstly, they need to be able to have RPCs executed on them. In other words, they must respond to requests. Secondly, they need to periodically execute stabilization routines, such that their routing information is kept up to date. To achieve this, E-Chord uses a threading approach. Specifically, each E-Chord node executes three basic threads. There is the main thread, the stabilizer thread and the listener thread.","title":"Main architecture"},{"location":"discussion/architecture/#the-listener-thread","text":"Firstly, let's focus on the listener thread. The purpose of this thread is to host the server the node is running, listening for incoming requests. When this thread receives a request, it creates a new thread to handle that request. The reason we don't just handle the request in the listener itself becomes apparent when you consider a situation, in which many requests are received by a node at once. Imagine a node receiving many different requests at the same time. If we handled each of them individually in the listener thread, we'd have to handle them sequentially. Remember that handling a request might require making additional requests to other nodes, in a recursive manner. This is a problem, because what if the first request took a whole second to complete? That would mean the rest of the requests would need to wait in line before being handled. Instead, by starting a handler thread for each request, no request needs to wait for another to complete, in order to be handled. Requests that can be handled instantly (for example, a poll RPC) will be, whereas requests that might require more time (like a find_key RPC) won't block unrelated traffic.","title":"The listener thread"},{"location":"discussion/architecture/#shared-resources","text":"This subsections describes the function of a mutex. You may skip to the next section if you are already familiar with it. Consider an integer with the value 0. Two threads both want to increment the value of that variable. Each thread runs the following code: x += 1 This might look like a single line of code, but internally, it looks more like this: READ x ADD 1 WRITE x As you can see, this isn't just one step anymore. Now imagine having two threads running this code. Internally, the CPU only runs one instruction at a time. As such, one possible execution flow is the following: Thread 1 Thread 2 READ x (x = 0) READ x (x =0) ADD 1 (x = 1) ADD 1 (x = 1) WRITE x (x = 1) WRITE x (x = 1) If Thread 1 had written the value (x = 1) before Thread 2 had read it, Thread 2 would have read 1 and correctly incremented it to 2. Yet, the final value for the variable in memory is 1. Since we run two threads, each incrementing the variable by 1, we expected it to be 2. We refer to x here as a shared resource. The solution to these types of problems is to use a mutex. A mutex is a special type of variable that can be locked and unlocked. Once a thread calls lock on the mutex, any other thread calling lock on it must wait until the first thread calls unlock in order to proceed. In the above example, if Thread 1 locks a mutex before entering, Thread 2 will be stuck waiting for Thread 1 to unlock that mutex. Thread 1 will do so after writing the value 1. Therefore, Thread 2 will correctly read 1 and increment it to 2.","title":"Shared resources"},{"location":"discussion/architecture/#routing-information-as-a-shared-resource","text":"With the described system in place, we are able to respond to multiple requests in a non-blocking manner. However, this creates a different issue we must tackle. What happens when two requests both concern the same routing data in the node? For instance, what if two different requests are about the node's predecessor? In that case, that routing information becomes a shared resource. Two different requests (i.e two threads) need access to this resource at once. Such a situation needs to be handled with care. Consider one request that performs searching and another that updates routing information. If the latter performs the update while the former is executing, this could throw off the search algorithm and result in a routing failure. To combat this, E-Chord assigns an additional property to each of its threads. The main thread is considered a writer, while every other thread is considered a reader. These properties imply that only the main thread is able to alter any sort of data on the node. Other threads may only read that data, but never change it. To ensure this results in shared resources being handled correctly, we also employ the use of a Reader-Writer lock. This is a special type of mutex that allows any number of readers access to the shared resource at once, but only allows one writer access. On top of this, both the writer and readers cannot have simultaneous access. This works great, because it means that while a request is handled, no data on the node can be changed, since the thread handling it is a reader. Changes can only occur if the writer has access to the resource. In that case, we know none of the readers are executing, so we don't have to worry about routing issues related to the shared resources.","title":"Routing information as a shared resource"},{"location":"discussion/architecture/#the-main-thread","text":"Naturally, we expect that RPCs could change the data of a node. For instance, the update_predecessor RPC requires that a node update its predecessor node. Yet, the previous approach ensures that RPCs, which are always readers, can never change the node's data. How do we deal with this issue? As we saw, the main thread is considered the writer, so only it can make changes to data. Thus, any readers that want to make changes must somehow defer them, so they are made by the writer when it gets resource access. The method in which this happens utilizes the event queue. If a reader wants to make a change, it inserts a function that makes that change in the queue. Note that the function isn't executed, so the change is not made instantly. The main thread then retrieves data from that queue. If that data is a function, it means it is some change that a reader needed to make, so the main thread, being the writer, executes that function. Generally, the main thread repeatedly reads data from the queue. Once data is found, it requests access to the resource. When it gains access, it handles the data appropriately (for example, runs it if it is a function). Finally, it frees the resource so readers that are waiting can gain access to it, and repeats the process.","title":"The main thread"},{"location":"discussion/architecture/#the-stabilizer-thread","text":"The final thread of an E-Chord node is the stabilizer. This thread's function is simple. The thread waits for a certain amount of time, as described by the parameters, and then inserts a special token ( 0 ) into the queue. The stabilizer thread is also a reader and, thus, cannot run the stabilization procedures itself (since those require editing a node's data). By inserting the token, the main thread will know that stabilization needs to be run and can then run it itself.","title":"The stabilizer thread"},{"location":"discussion/architecture/#connection-pool","text":"By observing the Chord protocol, one might notice that there is a lot of network traffic used for stabilization. This ensures the network will always remain stable, but requires a large amount of bandwidth to achieve this. If we examine those requests further, we might notice that for a given network, a node will mostly only contact the same nodes periodically. If we were to open a new TCP socket every time we needed to make an RPC, there would be lots of unnecessary bandwidth overhead just for creating connections. E-Chord attempts to reduce that overhead by intelligently keeping connections alive, based on their rate of usage. Specifically, E-Chord uses a connection pool, whose purpose is to keep certain connections alive. This connection pool is a layer of abstraction. Whenever E-Chord makes a request, it defers that request to the connection pool. The connection pool determines whether an open connection exists with the target node and, if it does, uses that connection instead of opening a new one. Additionally, whenever stabilization routines are executed, we also execute cleanup routines for the connection pool, in order to close unused connections. The connection pool itself also handles issues related to synchronization. Since connections are kept open, multiple requests to the same node will use the same connection. Thus, open connections also become shared resources, so the connection pool handles this accordingly.","title":"Connection pool"},{"location":"discussion/architecture/#security","text":"If you're familiar with E-Chord's requests, you might notice that it would be very easy for a bad actor to disrupt the network. Any request received, if valid, is taken at face value by E-Chord nodes. In other words, it is assumed that all nodes in the network operate in an expected fashion. This is because E-Chord implements the overlay network itself, and doesn't concern itself with security. A second layer could theoretically be set up on top of it, which validates requests and ensures bad actors cannot disrupt network operation.","title":"Security"},{"location":"discussion/architecture/#backups","text":"E-Chord also does not implement data backups. This means that any node that leaves abruptly causes all the data stored on it to be lost. When it comes to implementing backups, various methods could be used. One such idea would be to use multiple hash functions, thereby creating multiple virtual rings, and store each pair on every ring, instead of just once. Of course, this would increase network bandwidth significantly.","title":"Backups"},{"location":"discussion/chord_1/","text":"The Chord protocol: a distributed hash space In this discussion page, we'll focus on the Chord protocol itself. We'll describe what the protocol does and give certain details on how it achieves this. At its core, the challenge faced with creating a distributed hash table can be described in two parts: How can we distribute the hash table between multiple nodes? How can we, from any one node, find the node that contains or should contain a key? In this page, we'll mostly restrict ourselves to the former question. For details on the latter, visit the next discussion page . Distributing a hash table To begin with, we'll look at how Chord distributes a hash table to multiple systems. Chord refers to each system in the network as a node. Before anything else, we must recognize the need for node identifiers. If we can't tell nodes apart from each other, we'll find it much harder to determine which node should contain which data. In a computer network, each system is identified by its address. As such, we already have a piece of information that is different between nodes. The Chord protocol takes this a step further, by creating an additional identifier for each node. We'll call this the node ID. The interesting thing about node IDs is that they exist in the hash space! In other words, we create these IDs by hashing the address of the node. The resulting ID has certain properties, the usefulness of which will become apparent once we look at the way the protocol routes requests. The Chord ring Imagine taking a hash table's array and connecting the two ends of it. You now have a ring with the same properties as the hash table itself. The only difference is that the last spot lies directly next to the first one. This is the idea behind Chord's hash space. To generalize, we won't think of it as a hash table. Instead, think of 2 r points, arranged in a circle. Number those points from 0 to 2 r - 1 in clockwise ascending order, starting at any one point. You'll notice point n has neighbors n - 1 and n + 1 , with the exception of 2 r - 1 and 0 , which have each other as neighbors. This virtual ring makes up Chord's hash space and is the basis for all operations in the Chord protocol. Nodes as part of the hash space To get an ID for a node, we use a hash function on the node's network address. The only requirement for the hash function is that it returns an integer in the range [0, 2 r - 1] , where r is the number of bits for each ID in the hash space. This means the node gets an ID that falls somewhere on the ring, corresponding to one of the points in it. Therefore, the node itself becomes part of the hash space. Distributing keys Now that we have defined our hash space and placed our nodes inside of it, we can move on to determining a split of it between the existing nodes. The Chord protocol states that any node is responsible for the range [p + 1, n] on the hash space, where n is the node's ID and p is the first node met if we traverse the ring counter-clockwise from n . As a result, whenever we need to insert a new pair into our DHT, we hash the key. If the resulting hash k is in the range [p + 1, n] , we store the pair on node n . More rigorously, we find the range [p + 1, n] into which k belongs and store the pair on n . Similarly, if we want to retrieve a pair, we hash the key, get the hash value k and ask node n , such that k \u2208 [p + 1, n] . In fact, this is the reason the protocol is called Chord. Each node in the network is responsible for a specific chord of the ring. With this, we have answered the first question concerning the creation of a DHT. Key distribution When using a hash function, we prefer keys to be distributed evenly over the hash space. If this doesn't happen, it can cause collisions that result in operations being slower than expected. With Chord, one such instance can be observed when the network consists of few nodes that are close to each other in hash space. Take, for instance, the sample execution from the adding data tutorial : \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 9 Stability: 100.00% In this example, we use a ring with 14-bit IDs. This means there are 16,384 possible IDs in the network. As we saw, every node is responsible for the IDs between the previous node ID and its own node ID. Let's calculate which ID ranges each of the nodes in this example is responsible for. 2503: 15627 to 2503, total (16383 - 15626) + 2504 = 3261 IDs. 8230: 2504 to 8230, total 8230 - 2503 = 5727 IDs. 15626: 8231 to 15626, total 15626 - 8230 = 7396 IDs. Notice that 15626 is responsible for a much larger number of IDs than the other two. Assuming the hash function is equally likely to output any ID, it is not surprising that more keys are stored in node 15626. Yet, 7 as opposed to 1 for each of the others is still more disproportional than expected. If you'd like to get a better example, simply run E-Chord with many nodes and insert a large number of keys. Observe the nodes which are responsible for really small intervals, as opposed to those responsible for large ones. You'll find that the former store a significantly smaller number of IDs than the latter.","title":"The Chord protocol: a distributed hash space"},{"location":"discussion/chord_1/#the-chord-protocol-a-distributed-hash-space","text":"In this discussion page, we'll focus on the Chord protocol itself. We'll describe what the protocol does and give certain details on how it achieves this. At its core, the challenge faced with creating a distributed hash table can be described in two parts: How can we distribute the hash table between multiple nodes? How can we, from any one node, find the node that contains or should contain a key? In this page, we'll mostly restrict ourselves to the former question. For details on the latter, visit the next discussion page .","title":"The Chord protocol: a distributed hash space"},{"location":"discussion/chord_1/#distributing-a-hash-table","text":"To begin with, we'll look at how Chord distributes a hash table to multiple systems. Chord refers to each system in the network as a node. Before anything else, we must recognize the need for node identifiers. If we can't tell nodes apart from each other, we'll find it much harder to determine which node should contain which data. In a computer network, each system is identified by its address. As such, we already have a piece of information that is different between nodes. The Chord protocol takes this a step further, by creating an additional identifier for each node. We'll call this the node ID. The interesting thing about node IDs is that they exist in the hash space! In other words, we create these IDs by hashing the address of the node. The resulting ID has certain properties, the usefulness of which will become apparent once we look at the way the protocol routes requests.","title":"Distributing a hash table"},{"location":"discussion/chord_1/#the-chord-ring","text":"Imagine taking a hash table's array and connecting the two ends of it. You now have a ring with the same properties as the hash table itself. The only difference is that the last spot lies directly next to the first one. This is the idea behind Chord's hash space. To generalize, we won't think of it as a hash table. Instead, think of 2 r points, arranged in a circle. Number those points from 0 to 2 r - 1 in clockwise ascending order, starting at any one point. You'll notice point n has neighbors n - 1 and n + 1 , with the exception of 2 r - 1 and 0 , which have each other as neighbors. This virtual ring makes up Chord's hash space and is the basis for all operations in the Chord protocol.","title":"The Chord ring"},{"location":"discussion/chord_1/#nodes-as-part-of-the-hash-space","text":"To get an ID for a node, we use a hash function on the node's network address. The only requirement for the hash function is that it returns an integer in the range [0, 2 r - 1] , where r is the number of bits for each ID in the hash space. This means the node gets an ID that falls somewhere on the ring, corresponding to one of the points in it. Therefore, the node itself becomes part of the hash space.","title":"Nodes as part of the hash space"},{"location":"discussion/chord_1/#distributing-keys","text":"Now that we have defined our hash space and placed our nodes inside of it, we can move on to determining a split of it between the existing nodes. The Chord protocol states that any node is responsible for the range [p + 1, n] on the hash space, where n is the node's ID and p is the first node met if we traverse the ring counter-clockwise from n . As a result, whenever we need to insert a new pair into our DHT, we hash the key. If the resulting hash k is in the range [p + 1, n] , we store the pair on node n . More rigorously, we find the range [p + 1, n] into which k belongs and store the pair on n . Similarly, if we want to retrieve a pair, we hash the key, get the hash value k and ask node n , such that k \u2208 [p + 1, n] . In fact, this is the reason the protocol is called Chord. Each node in the network is responsible for a specific chord of the ring. With this, we have answered the first question concerning the creation of a DHT.","title":"Distributing keys"},{"location":"discussion/chord_1/#key-distribution","text":"When using a hash function, we prefer keys to be distributed evenly over the hash space. If this doesn't happen, it can cause collisions that result in operations being slower than expected. With Chord, one such instance can be observed when the network consists of few nodes that are close to each other in hash space. Take, for instance, the sample execution from the adding data tutorial : \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 9 Stability: 100.00% In this example, we use a ring with 14-bit IDs. This means there are 16,384 possible IDs in the network. As we saw, every node is responsible for the IDs between the previous node ID and its own node ID. Let's calculate which ID ranges each of the nodes in this example is responsible for. 2503: 15627 to 2503, total (16383 - 15626) + 2504 = 3261 IDs. 8230: 2504 to 8230, total 8230 - 2503 = 5727 IDs. 15626: 8231 to 15626, total 15626 - 8230 = 7396 IDs. Notice that 15626 is responsible for a much larger number of IDs than the other two. Assuming the hash function is equally likely to output any ID, it is not surprising that more keys are stored in node 15626. Yet, 7 as opposed to 1 for each of the others is still more disproportional than expected. If you'd like to get a better example, simply run E-Chord with many nodes and insert a large number of keys. Observe the nodes which are responsible for really small intervals, as opposed to those responsible for large ones. You'll find that the former store a significantly smaller number of IDs than the latter.","title":"Key distribution"},{"location":"discussion/chord_2/","text":"The Chord protocol: locating the right node Perhaps the hardest part of creating a DHT is being able to locate data that isn't saved locally, but rather on a different node. Each node needs to be able to determine the correct location of a key in the network, so that a pair can be saved to or retrieved from the network from anywhere. One simple solution for this would be to keep information about all nodes at a central location. However, this would make the system centralized and introduce a single point of failure. Here, we instead look for a solution that is decentralized, meaning no specific node's failure can cause the entire system to collapse. Routing-related data kept by nodes In order to be able to locate different nodes when necessary, nodes on the Chord network keep routing information. This consists of information about certain other nodes in the network. Specifically, each node keeps information about its successor, as well as an additional set of nodes, called the finger table. To create the finger table for a node n , we make a total of r clockwise jumps from n . The length of jump i is 2 i - 1 . As such, each jump will end up at a specific ID on the ring. We then find the node responsible for that ID. That node is either the node with the same ID, if it exists, or the next clockwise node after the ID, if it doesn't. We keep that node as finger i . The purpose of the finger table will be described later. Here, we should point out that the finger table allows for jumps of exponentially increasing size. This means that each node is likely to have information about other nodes that are roughly on the opposite side of the ring. Such information will allow us to shorten the total search steps required during routing. Each node also retains information about its predecessor. Searching for a key Consider an active Chord network, consisting of various nodes. We know the address of some random node in the network, and we'd like to add some new data, or search for some existing data. In both cases, we first need to ask the node to locate the correct node for the ID of our data. We contact our node and provide it with the key and value we'd like to search for. The node hashes the key and gets an ID in hash space. Once the node has the ID, how can it determine which node in the network is responsible for that ID? The node responsible for the ID will be the ID's successor. That is, the first node met if we move clockwise from the ID. To find this successor, we need to first find its predecessor and then ask it for its successor. Since nodes keep information about successors, finding the predecessor is equivalent to finding the node in question. To find the predecessor, we use the following algorithm. Set the current node to this node. Check if the ID is contained between the current node and the current node's successor on the ring. If it is, return the current node. If it is not, find the node pointed to by the longest finger in the finger table of the current node, which appears to the left of our ID (counter-clockwise) on the ring. Set that node as the current node. Return to step 2. With this algorithm, we can determine which node is the predecessor to an ID. Once we have this, we simply ask that node for its successor. That node is the one responsible for the ID. We can then ask that node to either save the data, or return any data it might already have. Joining the network as a new node You might assume that joining the network is a difficult process, because you'd need to accurately create the successor and finger table pointers. Yet, it turns out that when a node initially joins the network, there are only a few actions that must be completed. As with searching, to join the network, we must first know the address of some node in the network. This problem is solved by the seed server. Its purpose is to give us the address of some random node when asked. Once we obtain that address, we execute the following algorithm. Hash this computer's network address to get a node ID. Ask the seed node for the successor to that node ID. Set the node's successor to this node. Initialize the finger table, so they all point to this node. Inform the successor that this node is now its predecessor and ask it to move appropriate keys (those with an ID smaller than or equal to this node's) to this node. These steps are initially adequate. Although there are still various pointers that are now incorrect or have been invalidated, those will be fixed in time. How this happens is described in the next part. Network stabilization The previous protocol works great if every node already has correct pointers for its successor and finger table. However, Chord is a distributed system with nodes joining and leaving constantly. Therefore, a process by which these pointers are kept up to date is necessary. To keep all pointers up to date, Chord uses a few periodic stabilization routines. These are functions that run every few seconds and ensure that some of the node's pointers are correct, or fix them if they aren't. These routines can vary based on implementation details. In general, there two main types. The first one corrects successor and predecessor pointers, while the second one corrects finger table pointers. The general idea for the former is that each node periodically asks its successor for its predecessor. If its predecessor is different than the node itself, we know a new node has joined between this node and its successor. In that case, we simply update this node's successor to that node. When it comes to keeping fingers up to date, there are various methods. The best ones periodically check for the successor to n + 2 i , where i represents the i -th finger, and update the finger table accordingly. Here, it should be noted that there is a version of Chord that does not use stabilization routines. Instead, when a node joins, the protocol attempts to correctly update all pointers for all existing nodes. This, however, causes massive network overhead for join operations and also cannot account for abrupt node failures. For this reason, most implementations will instead opt for the stabilization routine approach. Node failures The advantage stabilization routines provide becomes even more obvious when dealing with node failures. Since nodes periodically check their pointers, if a node dies, others in the network will sooner or later determine this and update their pointers on their own. This means that we don't need to do anything else to deal with node failures. The network is self-stabilizing, so we only need to give it time to return to a stable state after nodes fail. Abrupt node failures, of course, will cause all data stored on a node to be lost. To avoid this, it is necessary to implement some sort of backup system, such that certain keys are not only stored on the successor node to their ID, but also on a few others, thus reducing the probability of data loss in case of node failure.","title":"The Chord protocol: locating the right node"},{"location":"discussion/chord_2/#the-chord-protocol-locating-the-right-node","text":"Perhaps the hardest part of creating a DHT is being able to locate data that isn't saved locally, but rather on a different node. Each node needs to be able to determine the correct location of a key in the network, so that a pair can be saved to or retrieved from the network from anywhere. One simple solution for this would be to keep information about all nodes at a central location. However, this would make the system centralized and introduce a single point of failure. Here, we instead look for a solution that is decentralized, meaning no specific node's failure can cause the entire system to collapse.","title":"The Chord protocol: locating the right node"},{"location":"discussion/chord_2/#routing-related-data-kept-by-nodes","text":"In order to be able to locate different nodes when necessary, nodes on the Chord network keep routing information. This consists of information about certain other nodes in the network. Specifically, each node keeps information about its successor, as well as an additional set of nodes, called the finger table. To create the finger table for a node n , we make a total of r clockwise jumps from n . The length of jump i is 2 i - 1 . As such, each jump will end up at a specific ID on the ring. We then find the node responsible for that ID. That node is either the node with the same ID, if it exists, or the next clockwise node after the ID, if it doesn't. We keep that node as finger i . The purpose of the finger table will be described later. Here, we should point out that the finger table allows for jumps of exponentially increasing size. This means that each node is likely to have information about other nodes that are roughly on the opposite side of the ring. Such information will allow us to shorten the total search steps required during routing. Each node also retains information about its predecessor.","title":"Routing-related data kept by nodes"},{"location":"discussion/chord_2/#searching-for-a-key","text":"Consider an active Chord network, consisting of various nodes. We know the address of some random node in the network, and we'd like to add some new data, or search for some existing data. In both cases, we first need to ask the node to locate the correct node for the ID of our data. We contact our node and provide it with the key and value we'd like to search for. The node hashes the key and gets an ID in hash space. Once the node has the ID, how can it determine which node in the network is responsible for that ID? The node responsible for the ID will be the ID's successor. That is, the first node met if we move clockwise from the ID. To find this successor, we need to first find its predecessor and then ask it for its successor. Since nodes keep information about successors, finding the predecessor is equivalent to finding the node in question. To find the predecessor, we use the following algorithm. Set the current node to this node. Check if the ID is contained between the current node and the current node's successor on the ring. If it is, return the current node. If it is not, find the node pointed to by the longest finger in the finger table of the current node, which appears to the left of our ID (counter-clockwise) on the ring. Set that node as the current node. Return to step 2. With this algorithm, we can determine which node is the predecessor to an ID. Once we have this, we simply ask that node for its successor. That node is the one responsible for the ID. We can then ask that node to either save the data, or return any data it might already have.","title":"Searching for a key"},{"location":"discussion/chord_2/#joining-the-network-as-a-new-node","text":"You might assume that joining the network is a difficult process, because you'd need to accurately create the successor and finger table pointers. Yet, it turns out that when a node initially joins the network, there are only a few actions that must be completed. As with searching, to join the network, we must first know the address of some node in the network. This problem is solved by the seed server. Its purpose is to give us the address of some random node when asked. Once we obtain that address, we execute the following algorithm. Hash this computer's network address to get a node ID. Ask the seed node for the successor to that node ID. Set the node's successor to this node. Initialize the finger table, so they all point to this node. Inform the successor that this node is now its predecessor and ask it to move appropriate keys (those with an ID smaller than or equal to this node's) to this node. These steps are initially adequate. Although there are still various pointers that are now incorrect or have been invalidated, those will be fixed in time. How this happens is described in the next part.","title":"Joining the network as a new node"},{"location":"discussion/chord_2/#network-stabilization","text":"The previous protocol works great if every node already has correct pointers for its successor and finger table. However, Chord is a distributed system with nodes joining and leaving constantly. Therefore, a process by which these pointers are kept up to date is necessary. To keep all pointers up to date, Chord uses a few periodic stabilization routines. These are functions that run every few seconds and ensure that some of the node's pointers are correct, or fix them if they aren't. These routines can vary based on implementation details. In general, there two main types. The first one corrects successor and predecessor pointers, while the second one corrects finger table pointers. The general idea for the former is that each node periodically asks its successor for its predecessor. If its predecessor is different than the node itself, we know a new node has joined between this node and its successor. In that case, we simply update this node's successor to that node. When it comes to keeping fingers up to date, there are various methods. The best ones periodically check for the successor to n + 2 i , where i represents the i -th finger, and update the finger table accordingly. Here, it should be noted that there is a version of Chord that does not use stabilization routines. Instead, when a node joins, the protocol attempts to correctly update all pointers for all existing nodes. This, however, causes massive network overhead for join operations and also cannot account for abrupt node failures. For this reason, most implementations will instead opt for the stabilization routine approach.","title":"Network stabilization"},{"location":"discussion/chord_2/#node-failures","text":"The advantage stabilization routines provide becomes even more obvious when dealing with node failures. Since nodes periodically check their pointers, if a node dies, others in the network will sooner or later determine this and update their pointers on their own. This means that we don't need to do anything else to deal with node failures. The network is self-stabilizing, so we only need to give it time to return to a stable state after nodes fail. Abrupt node failures, of course, will cause all data stored on a node to be lost. To avoid this, it is necessary to implement some sort of backup system, such that certain keys are not only stored on the successor node to their ID, but also on a few others, thus reducing the probability of data loss in case of node failure.","title":"Node failures"},{"location":"discussion/e-chord/","text":"E-Chord: An implementation of Chord E-Chord is an implementation of the Chord protocol. Specifically, we implement the version of Chord that utilizes stabilization routines. We refer to this version of Chord as asynchronous, as opposed to the synchronous version. This is because, as described in the previous discussion , this version of the protocol allows a node to join and slowly build its pointers, as opposed to immediately blocking the network until all pointers can be updated. Seed server E-Chord uses a seed server to keep track of nodes in the network. This serves a few purposes. Mainly, a node asks the seed server for the address of another in order to join the network. Without the seed server, this would only be possible by using a fixed address for some node, which would likely be infeasible in most systems. Additionally, after a massive node failure, if a node is unable to locate any others through its pointers, it will ask the seed server for the address of a node in order to re-join the network. In practice, due to the types of pointers an E-Chord node holds, this situation is statistically impossible. Finally, the seed server can be used by an outside client to learn the address of some node, in order to then perform a query. It might seem like using a seed server makes the network less centralized. However, this is not the case. The seed server's only job is providing initial contact information for the network. Beyond that, it does not affect network operation at all, nor is it required for any purpose. In fact, after a network is running, the seed server can be shut down. Of course, this means that new nodes won't be able to join the network, but the network itself will continue regular operation. Routing data The routing information kept by E-Chord nodes includes the main types of information described by the Chord protocol. Specifically, each node keeps a finger table of size r , where r is the number of bits of IDs in the ring. Each node also keeps its predecessor. Additionally, instead of just a single successor pointer, each node keeps a successor list of size r . This is done so, if a massive node failure occurs, it is highly unlikely that some node will be cut off from the network completely. All of this data is kept up to date by stabilization routines. Stabilization An E-Chord stabilize event is made up of three parts: stabilize , fix_successor_list and fix_fingers . All of these take into account various edge cases where pointers might point to dead nodes. Here, we'll only describe their basic function, while ignoring these details. The first part, stabilize , works as described by the Chord protocol. A node asks its successor for its predecessor and updates its own successor accordingly. The second part, fix_successor_list , uses a custom algorithm created for this purpose. The algorithm keeps the successor list, as well as a current index on the list. A description of the algorithm follows. If successor list empty or index<0, ask successor for successor; if successor dead, end; if successor alive, place response at start of list (override or append); if index<0, index=0; end Else, ask current index node in successor list for successor If alive and last in list and list is full, index=-1; end If alive and last in list and list not full, append new successor; index+=1; end If alive and not last in list, verify next; if same, index+=1; end; else if different, override next; index+=1; end If dead, remove node from successor list; index-=1; end This algorithm ensures that the successor list remains up to date. It also serves to build an empty successor list, meaning that new nodes can simply initialize their successor list to empty. Finally, fix_fingers picks a random finger from the finger table and updates it. It does this by searching for the successor of the key n + 2 i , where i is the index of the finger, and using that as the new finger. Local simulation E-Chord comes with scripts that allow users to simulate an E-Chord network on a single machine. We refer to a network created in such a manner as a simulation. Besides the fact that the network is running on a single computer, there isn't much difference in the function of a simulated network. When a node is started on a computer, it creates a process that runs this node. Through the local network, this process communicates with other nodes and thus makes up the DHT. When executing the scripts, multiple such processes are started on the same computer. They all use the IP address localhost , while they listen on different ports. The visualizer script itself could be modified, so that it works for actual E-Chord networks. To do this, one would simply have to provide it with a list of addresses of existing nodes that it can make requests to. As it now stands, the visualizer makes requests to nodes with localhost IP addresses that use ports in the range defined by the testing parameters.","title":"E-Chord: An implementation of Chord"},{"location":"discussion/e-chord/#e-chord-an-implementation-of-chord","text":"E-Chord is an implementation of the Chord protocol. Specifically, we implement the version of Chord that utilizes stabilization routines. We refer to this version of Chord as asynchronous, as opposed to the synchronous version. This is because, as described in the previous discussion , this version of the protocol allows a node to join and slowly build its pointers, as opposed to immediately blocking the network until all pointers can be updated.","title":"E-Chord: An implementation of Chord"},{"location":"discussion/e-chord/#seed-server","text":"E-Chord uses a seed server to keep track of nodes in the network. This serves a few purposes. Mainly, a node asks the seed server for the address of another in order to join the network. Without the seed server, this would only be possible by using a fixed address for some node, which would likely be infeasible in most systems. Additionally, after a massive node failure, if a node is unable to locate any others through its pointers, it will ask the seed server for the address of a node in order to re-join the network. In practice, due to the types of pointers an E-Chord node holds, this situation is statistically impossible. Finally, the seed server can be used by an outside client to learn the address of some node, in order to then perform a query. It might seem like using a seed server makes the network less centralized. However, this is not the case. The seed server's only job is providing initial contact information for the network. Beyond that, it does not affect network operation at all, nor is it required for any purpose. In fact, after a network is running, the seed server can be shut down. Of course, this means that new nodes won't be able to join the network, but the network itself will continue regular operation.","title":"Seed server"},{"location":"discussion/e-chord/#routing-data","text":"The routing information kept by E-Chord nodes includes the main types of information described by the Chord protocol. Specifically, each node keeps a finger table of size r , where r is the number of bits of IDs in the ring. Each node also keeps its predecessor. Additionally, instead of just a single successor pointer, each node keeps a successor list of size r . This is done so, if a massive node failure occurs, it is highly unlikely that some node will be cut off from the network completely. All of this data is kept up to date by stabilization routines.","title":"Routing data"},{"location":"discussion/e-chord/#stabilization","text":"An E-Chord stabilize event is made up of three parts: stabilize , fix_successor_list and fix_fingers . All of these take into account various edge cases where pointers might point to dead nodes. Here, we'll only describe their basic function, while ignoring these details. The first part, stabilize , works as described by the Chord protocol. A node asks its successor for its predecessor and updates its own successor accordingly. The second part, fix_successor_list , uses a custom algorithm created for this purpose. The algorithm keeps the successor list, as well as a current index on the list. A description of the algorithm follows. If successor list empty or index<0, ask successor for successor; if successor dead, end; if successor alive, place response at start of list (override or append); if index<0, index=0; end Else, ask current index node in successor list for successor If alive and last in list and list is full, index=-1; end If alive and last in list and list not full, append new successor; index+=1; end If alive and not last in list, verify next; if same, index+=1; end; else if different, override next; index+=1; end If dead, remove node from successor list; index-=1; end This algorithm ensures that the successor list remains up to date. It also serves to build an empty successor list, meaning that new nodes can simply initialize their successor list to empty. Finally, fix_fingers picks a random finger from the finger table and updates it. It does this by searching for the successor of the key n + 2 i , where i is the index of the finger, and using that as the new finger.","title":"Stabilization"},{"location":"discussion/e-chord/#local-simulation","text":"E-Chord comes with scripts that allow users to simulate an E-Chord network on a single machine. We refer to a network created in such a manner as a simulation. Besides the fact that the network is running on a single computer, there isn't much difference in the function of a simulated network. When a node is started on a computer, it creates a process that runs this node. Through the local network, this process communicates with other nodes and thus makes up the DHT. When executing the scripts, multiple such processes are started on the same computer. They all use the IP address localhost , while they listen on different ports. The visualizer script itself could be modified, so that it works for actual E-Chord networks. To do this, one would simply have to provide it with a list of addresses of existing nodes that it can make requests to. As it now stands, the visualizer makes requests to nodes with localhost IP addresses that use ports in the range defined by the testing parameters.","title":"Local simulation"},{"location":"discussion/hashing/","text":"The Hash Table Before delving into how a distributed hash table can function, it is imperative that one understands how hashing itself works. In this page, we'll describe hashing at its most basic level. What is hashing? Hashing is one of the most fundamental concepts in Computer Science. From data storage, to cryptography and security, to algorithms and much more, pretty much every sector of Computer Science utilizes hashing in some way or another. At its core, hashing amounts to mapping data from an arbitrarily large space to a fixed size space. In other words, it transforms some input, no matter how small or large, to an input of a specific size and format. Hash functions Hashing is performed using hash functions. As described above, hashing is a transformation of an input from some space to another, therefore a function is the tool we're looking for to achieve this. A hash function accepts data in some format (for instance, strings) and returns what we call a hash digest, or simply hash. A hash can be an integer, a string, or any other data type, but in its most general form, it is data (bytes). Let's look at an example of a simple hash function. We'd like to provide some string of arbitrary size as an input, but get an integer in the range [0-8] as an output. The idea behind this hash function is simple: we'll assign a number to each character in the string, sum up the numbers and get the result of that sum modulo 8. This will result in an integer between 0 and 7. For the sake of argument, let's assign 1 to the character a and 0 to every other character. Let's call this new hash function H 1 . We'll now apply our newly created hash function to some inputs: H 1 (\"Mary\") = 1 H 1 (\"John\") = 0 H 1 (\"Elise\") = 0 H 1 (\"Eleanor\") = 1 Notice how multiple inputs can give the same output? We call this a collision and it is a fundamental problem in hashing. Hash function quality In fact, when attempting to map inputs from a space of infinite size to outputs from a space of finite size, it's obvious that most often, every output will have infinite inputs mapped to it. This is simply an unavoidable fact. This, however, does not mean that all hash functions are equal in quality. Since we'd rather not have any collisions, we rank the quality of a hash function based on how many collisions it produces (with relation to our input of interest). Take a look at our hash function from before. You might think that this isn't a good hash function and, for most inputs, you'd probably be right. But consider your input space being strings which contain an arbitrary number of a s. For such input, this hash function is optimal. In general, however, a hash function like this wouldn't be very useful. A much better one we can use here would assign each character its integer Unicode value. Let's call that function H 2 . Using this hash function, here are the values for the same names: H 2 (\"Mary\") = 1 H 2 (\"John\") = 7 H 2 (\"Elise\") = 2 H 2 (\"Eleanor\") = 6 Now, every name has its own value. This produces no collisions, which is a significantly better result than that of H 1 . Hash tables A hash table is a data structure that stores data in key-value pairs. It does this by using a hash function to transform the key, then using the transformed key to find the location where the value is saved. If this sounds complicated, let's take a look at an example. Assume the four names we used above are those of students. Each of them has received a grade in our class, so we want to save their names along with their grades, such that when one of them asks us for their grade, we can quickly tell them what it is. We'll use our hash function from before, H 2 . Because H 2 can produce 8 possible values, we'll create an array of size 8. After this, we'll apply H 2 to each of the names, which gives us the numbers seen above. We'll use each number as an index to the array, saving both the name and the grade at that position. 0 1 2 3 4 5 6 7 Mary: C Elise: B Eleanor: A John: B The next time one of these students asks us for their grade, we can simply hash their name, instantly go to the position the resulting hash indicates and get their grade. This might not seem like it's much of an improvement, but imagine having thousands of students instead of just four. If your hash table was large enough, you could get the grade for one pretty much instantly. The alternative would be looking through a list of thousands of students, which would certainly take longer. Collisions We mentioned before that we don't like collisions. Why is that? The answer to that question depends on your use case. To understand this for hash tables, we'll add another student to our example. Let's assume Jared also took the test and was graded. We want to add him to our hash table, so the first step is hashing his name. H 2 (\"Jared\") = 6 We notice that the value we got for his name is the same as that for Eleanor's. If we go to position 6 of our hash table, we've already saved Eleanor's grade there. There are multiple different approaches to solve this issue, but we'll use the one called chaining. This entails simply adding his name after hers, in a list, at that same position. The resulting array looks like this: 0 1 2 3 4 5 6 7 Mary: C Elise: B Eleanor: A Jared: B John: B Even though this works, it creates a certain problem. When Jared asks us for his grade, we hash his name and end up in position 6. There, we notice that the first grade is not his, but Eleanor's. We need to keep looking through the list to find his name and give him his grade. This makes it so that the process takes longer than it otherwise would. This is the reason we want to avoid collisions. Each collision means that two different keys get the same value and, for a hash table, this increases the time it takes to look for a key's value. How to deal with collisions The most common approach to dealing with collisions is simply increasing the output space. If, for instance, our hash function returned the sum modulo 128 and our array was size 128, each student in our example would get their own unique position in the hash table. This approach works, but only if our hash function is decent enough to distribute inputs over the output space evenly. If we used H 1 instead, we'd still have multiple collisions, even if we increased the array size significantly. In general, collisions are unavoidable, especially the more data we add. However, to keep the hash table's search functionality close to linear, we try to have a balance between hash function quality and hash table size. Why hash tables? We already saw how a hash table makes searching for a student's grade much faster in our previous example. Let's define the time and space complexity of a hash table more rigorously. Operation Average Case Worst Case Search \u0398(1) O(n) Insert \u0398(1) O(n) Delete \u0398(1) O(n) Even though the worst case complexity for these operations is the same as linear search in an array, the average case significantly improves this. In practice, this makes hash tables invaluable, with few other data structures being able to perform these functions as efficiently.","title":"The Hash Table"},{"location":"discussion/hashing/#the-hash-table","text":"Before delving into how a distributed hash table can function, it is imperative that one understands how hashing itself works. In this page, we'll describe hashing at its most basic level.","title":"The Hash Table"},{"location":"discussion/hashing/#what-is-hashing","text":"Hashing is one of the most fundamental concepts in Computer Science. From data storage, to cryptography and security, to algorithms and much more, pretty much every sector of Computer Science utilizes hashing in some way or another. At its core, hashing amounts to mapping data from an arbitrarily large space to a fixed size space. In other words, it transforms some input, no matter how small or large, to an input of a specific size and format.","title":"What is hashing?"},{"location":"discussion/hashing/#hash-functions","text":"Hashing is performed using hash functions. As described above, hashing is a transformation of an input from some space to another, therefore a function is the tool we're looking for to achieve this. A hash function accepts data in some format (for instance, strings) and returns what we call a hash digest, or simply hash. A hash can be an integer, a string, or any other data type, but in its most general form, it is data (bytes). Let's look at an example of a simple hash function. We'd like to provide some string of arbitrary size as an input, but get an integer in the range [0-8] as an output. The idea behind this hash function is simple: we'll assign a number to each character in the string, sum up the numbers and get the result of that sum modulo 8. This will result in an integer between 0 and 7. For the sake of argument, let's assign 1 to the character a and 0 to every other character. Let's call this new hash function H 1 . We'll now apply our newly created hash function to some inputs: H 1 (\"Mary\") = 1 H 1 (\"John\") = 0 H 1 (\"Elise\") = 0 H 1 (\"Eleanor\") = 1 Notice how multiple inputs can give the same output? We call this a collision and it is a fundamental problem in hashing.","title":"Hash functions"},{"location":"discussion/hashing/#hash-function-quality","text":"In fact, when attempting to map inputs from a space of infinite size to outputs from a space of finite size, it's obvious that most often, every output will have infinite inputs mapped to it. This is simply an unavoidable fact. This, however, does not mean that all hash functions are equal in quality. Since we'd rather not have any collisions, we rank the quality of a hash function based on how many collisions it produces (with relation to our input of interest). Take a look at our hash function from before. You might think that this isn't a good hash function and, for most inputs, you'd probably be right. But consider your input space being strings which contain an arbitrary number of a s. For such input, this hash function is optimal. In general, however, a hash function like this wouldn't be very useful. A much better one we can use here would assign each character its integer Unicode value. Let's call that function H 2 . Using this hash function, here are the values for the same names: H 2 (\"Mary\") = 1 H 2 (\"John\") = 7 H 2 (\"Elise\") = 2 H 2 (\"Eleanor\") = 6 Now, every name has its own value. This produces no collisions, which is a significantly better result than that of H 1 .","title":"Hash function quality"},{"location":"discussion/hashing/#hash-tables","text":"A hash table is a data structure that stores data in key-value pairs. It does this by using a hash function to transform the key, then using the transformed key to find the location where the value is saved. If this sounds complicated, let's take a look at an example. Assume the four names we used above are those of students. Each of them has received a grade in our class, so we want to save their names along with their grades, such that when one of them asks us for their grade, we can quickly tell them what it is. We'll use our hash function from before, H 2 . Because H 2 can produce 8 possible values, we'll create an array of size 8. After this, we'll apply H 2 to each of the names, which gives us the numbers seen above. We'll use each number as an index to the array, saving both the name and the grade at that position. 0 1 2 3 4 5 6 7 Mary: C Elise: B Eleanor: A John: B The next time one of these students asks us for their grade, we can simply hash their name, instantly go to the position the resulting hash indicates and get their grade. This might not seem like it's much of an improvement, but imagine having thousands of students instead of just four. If your hash table was large enough, you could get the grade for one pretty much instantly. The alternative would be looking through a list of thousands of students, which would certainly take longer.","title":"Hash tables"},{"location":"discussion/hashing/#collisions","text":"We mentioned before that we don't like collisions. Why is that? The answer to that question depends on your use case. To understand this for hash tables, we'll add another student to our example. Let's assume Jared also took the test and was graded. We want to add him to our hash table, so the first step is hashing his name. H 2 (\"Jared\") = 6 We notice that the value we got for his name is the same as that for Eleanor's. If we go to position 6 of our hash table, we've already saved Eleanor's grade there. There are multiple different approaches to solve this issue, but we'll use the one called chaining. This entails simply adding his name after hers, in a list, at that same position. The resulting array looks like this: 0 1 2 3 4 5 6 7 Mary: C Elise: B Eleanor: A Jared: B John: B Even though this works, it creates a certain problem. When Jared asks us for his grade, we hash his name and end up in position 6. There, we notice that the first grade is not his, but Eleanor's. We need to keep looking through the list to find his name and give him his grade. This makes it so that the process takes longer than it otherwise would. This is the reason we want to avoid collisions. Each collision means that two different keys get the same value and, for a hash table, this increases the time it takes to look for a key's value.","title":"Collisions"},{"location":"discussion/hashing/#how-to-deal-with-collisions","text":"The most common approach to dealing with collisions is simply increasing the output space. If, for instance, our hash function returned the sum modulo 128 and our array was size 128, each student in our example would get their own unique position in the hash table. This approach works, but only if our hash function is decent enough to distribute inputs over the output space evenly. If we used H 1 instead, we'd still have multiple collisions, even if we increased the array size significantly. In general, collisions are unavoidable, especially the more data we add. However, to keep the hash table's search functionality close to linear, we try to have a balance between hash function quality and hash table size.","title":"How to deal with collisions"},{"location":"discussion/hashing/#why-hash-tables","text":"We already saw how a hash table makes searching for a student's grade much faster in our previous example. Let's define the time and space complexity of a hash table more rigorously. Operation Average Case Worst Case Search \u0398(1) O(n) Insert \u0398(1) O(n) Delete \u0398(1) O(n) Even though the worst case complexity for these operations is the same as linear search in an array, the average case significantly improves this. In practice, this makes hash tables invaluable, with few other data structures being able to perform these functions as efficiently.","title":"Why hash tables?"},{"location":"discussion/overview/","text":"Overview Data storage, in its various forms, is one of the most basic concepts in computer science. From volatile memory, used by every single modern computing device to operate, to non-volatile memory, used for long term data storage, memory is one of the most important parts of any computer. As such, any programmer will tell you that writing code consists, in large part, of considering the correct ways to store data, thus allowing for efficient manipulation of it. Such methods of organized data storage are usually referred to as data structures. One of those structures, which countless algorithms rely on, is the hash table. When it comes to actually storing data in a computer's memory, every programming language will provide many useful options. For instance, every major language will already have some implementation for a hash table, such that using it in one's own code requires but a few lines of code at most. Nevertheless, as technology advances, applications require more and more storage space to cope with increasing dataset sizes. This presents a problem, because processing and storing such large data is infeasible for single computer systems. As such, the concept of distributed systems emerges, which aim to pool the processing and storage capacity of multiple systems to achieve this. Despite the obvious advantages of this approach, attempting this creates a new set of challenges. How can multiple systems work together to efficiently handle data, such that storage, processing and retrieval are all performed quickly and seamlessly, and there is an actual advantage to this over the single system variant? An answer to (some of) those questions is given by the Chord protocol. Chord is an algorithm for a peer-to-peer, distributed hash table. It describes the method by which data should be stored and retrieved in a system consisting of multiple computers (nodes). It is called a DHT (distributed hash table) because it stores data in key-value pairs, similar to how a regular hash table would do this in a single computer's memory. E-Chord is an implementation of this protocol. It performs the regular operations described by the protocol, while augmenting it with some additional features.","title":"Overview"},{"location":"discussion/overview/#overview","text":"Data storage, in its various forms, is one of the most basic concepts in computer science. From volatile memory, used by every single modern computing device to operate, to non-volatile memory, used for long term data storage, memory is one of the most important parts of any computer. As such, any programmer will tell you that writing code consists, in large part, of considering the correct ways to store data, thus allowing for efficient manipulation of it. Such methods of organized data storage are usually referred to as data structures. One of those structures, which countless algorithms rely on, is the hash table. When it comes to actually storing data in a computer's memory, every programming language will provide many useful options. For instance, every major language will already have some implementation for a hash table, such that using it in one's own code requires but a few lines of code at most. Nevertheless, as technology advances, applications require more and more storage space to cope with increasing dataset sizes. This presents a problem, because processing and storing such large data is infeasible for single computer systems. As such, the concept of distributed systems emerges, which aim to pool the processing and storage capacity of multiple systems to achieve this. Despite the obvious advantages of this approach, attempting this creates a new set of challenges. How can multiple systems work together to efficiently handle data, such that storage, processing and retrieval are all performed quickly and seamlessly, and there is an actual advantage to this over the single system variant? An answer to (some of) those questions is given by the Chord protocol. Chord is an algorithm for a peer-to-peer, distributed hash table. It describes the method by which data should be stored and retrieved in a system consisting of multiple computers (nodes). It is called a DHT (distributed hash table) because it stores data in key-value pairs, similar to how a regular hash table would do this in a single computer's memory. E-Chord is an implementation of this protocol. It performs the regular operations described by the protocol, while augmenting it with some additional features.","title":"Overview"},{"location":"how-to-guides/communication/","text":"How to communicate with an E-Chord network This guide describes the process by which you can communicate with an active E-Chord network. Connection In order to connect to the network, you need the address of some node that is part of it. This consists of a tuple of the IP address and the port number of the node. To connect to the network, open a TCP socket to that address. Requests Once you've done this, you are expected to send a request. E-Chord refers to any network message that asks a node to perform an action as a request. Requests are serialized JSON objects of the following format: { \"header\": { \"type\": TYPE }, \"body\": { ... } } The request header needs to contain a request type. TYPE is a string representing that type. Each request type has certain parameters that need to be included with it. Those should be placed in the request body, with their corresponding values. For a full list of available request types and their required parameters, refer to the communication reference guide . Note that only the first few request types, as pointed out by the reference guide, are meant for ordinary use. Others are debugging related, while the rest consist of internal requests that should normally only be used between the nodes themselves. Use those at your own risk. Responses E-Chord refers to anything returned to the sender after a request as a response. Responses generally follow the same format as a request: { \"header\": { \"status\": STATUS }, \"body\": { ... } } As such, responses are also serialized JSON objects. The response header always contains a status. STATUS is an integer, representing that status. If the request that triggered this response asked for data and data retrieval was successful according to the status, that data will be contained in the response body. The reference guide mentioned before also contains the expected response(s) for any given request type. Once you get a response to your request, you are generally free to close the connection.","title":"How to communicate with an E-Chord network"},{"location":"how-to-guides/communication/#how-to-communicate-with-an-e-chord-network","text":"This guide describes the process by which you can communicate with an active E-Chord network.","title":"How to communicate with an E-Chord network"},{"location":"how-to-guides/communication/#connection","text":"In order to connect to the network, you need the address of some node that is part of it. This consists of a tuple of the IP address and the port number of the node. To connect to the network, open a TCP socket to that address.","title":"Connection"},{"location":"how-to-guides/communication/#requests","text":"Once you've done this, you are expected to send a request. E-Chord refers to any network message that asks a node to perform an action as a request. Requests are serialized JSON objects of the following format: { \"header\": { \"type\": TYPE }, \"body\": { ... } } The request header needs to contain a request type. TYPE is a string representing that type. Each request type has certain parameters that need to be included with it. Those should be placed in the request body, with their corresponding values. For a full list of available request types and their required parameters, refer to the communication reference guide . Note that only the first few request types, as pointed out by the reference guide, are meant for ordinary use. Others are debugging related, while the rest consist of internal requests that should normally only be used between the nodes themselves. Use those at your own risk.","title":"Requests"},{"location":"how-to-guides/communication/#responses","text":"E-Chord refers to anything returned to the sender after a request as a response. Responses generally follow the same format as a request: { \"header\": { \"status\": STATUS }, \"body\": { ... } } As such, responses are also serialized JSON objects. The response header always contains a status. STATUS is an integer, representing that status. If the request that triggered this response asked for data and data retrieval was successful according to the status, that data will be contained in the response body. The reference guide mentioned before also contains the expected response(s) for any given request type. Once you get a response to your request, you are generally free to close the connection.","title":"Responses"},{"location":"how-to-guides/setting-up/","text":"How to set up an E-Chord network locally This guide describes the process by which an E-Chord network can be set up in a local network. Seed Server The first thing you'll need to do before starting any E-Chord nodes is to run a seed server. Unless you have a custom seed server, you can use the original implementation provided with the main E-Chord implementation. On the machine you want acting as a seed server, start by cloning the repository. At a location of your choice, open a terminal and use the command git clone https://github.com/notTypecast/E-Chord-Seed . You should see output similar to the below: Cloning into 'E-Chord-Seed'... remote: Enumerating objects: 48, done. remote: Counting objects: 100% (48/48), done. remote: Compressing objects: 100% (37/37), done. Receiving objects: 100% (48/48), 10.67 KiB | 214.00 KiB/s, done. remote: Total 48 (delta 24), reused 30 (delta 10), pack-reused 0 (from 0) Resolving deltas: 100% (24/24), done. If not, make sure git is installed on the system. Another way to get the repository is through GitHub's web interface itself. Above the repository's directory structure on the main page, you will find a button with the text <> Code . Click it, and choose the Download ZIP option. You should now have a ZIP file with the repository in it. Simply extract it at your location of choice. Once the repository has been cloned, open a terminal at its location. If you cloned the repository through the command line interface, using the command cd E-Chord-Seed directly after will suffice. Optionally, you can change the seed server parameters before starting the server. Open the params.json file. Here, you can change the server's port, which is 8000 by default. For a full list of parameters, refer to the parameter reference guide . Once finished, enter the command python3 server.py . If nothing goes wrong, you should see the below output: MainThread-INFO: Loaded params MainThread-INFO: Starting node on [your local IP]:[your PORT] You have now successfully set up the Seed Server! Nodes The next part is setting up the actual E-Chord network. On a computer you'd like to add to the network as a node, get the E-Chord repository, similarly to how you previously downloaded the seed repository. There is just one step that must be completed before entering the network. Open the config/params.json file. There, you'll find various parameters about the network that can be customized. For a full list of them, refer to the parameter reference guide . Under \"seed_server\" , you'll find the following parameters: \"seed_server\": { \"ip\": \"192.168.1.15\", \"port\": 8000, \"attempt_limit\": 4 } Make sure to change the seed server IP to the IP of the computer running the seed server on the local network. This was available at the [your local IP] part of the output after you ran the seed server. Similarly, change the port number to the number you used to run the seed server. Additionally, change the port number under host to a unique port number. Optionally, change any other parameters as needed. However, make sure that any changes are applied to every node in the network, since most of these parameters must be the same across all nodes. Again, refer to the guide for more information on individual parameters. Once the parameters are set up as required, you can launch the node on this system using the python3 start_node.py command. Do not provide the port as an argument! If you do this, the node will run on localhost and won't be visible to other computers in the network. If everything has been done correctly, you should see output confirming that the node has joined the network. If this is the first computer you've added to the network, the output should look somewhat like this: MainThread:11932-INFO: No other nodes in the network MainThread:11932-INFO: Initialized predecessor and sucessor list to self MainThread:11932-INFO: Starting node on [your local IP]:[your PORT] You'll need to repeat this process for each new computer you add to the network. As you add more computers, the output of each should start to indicate the presence of others in the network. Similarly, the output of the seed server should indicate awareness of each of the nodes joining the network.","title":"How to set up an E-Chord network locally"},{"location":"how-to-guides/setting-up/#how-to-set-up-an-e-chord-network-locally","text":"This guide describes the process by which an E-Chord network can be set up in a local network.","title":"How to set up an E-Chord network locally"},{"location":"how-to-guides/setting-up/#seed-server","text":"The first thing you'll need to do before starting any E-Chord nodes is to run a seed server. Unless you have a custom seed server, you can use the original implementation provided with the main E-Chord implementation. On the machine you want acting as a seed server, start by cloning the repository. At a location of your choice, open a terminal and use the command git clone https://github.com/notTypecast/E-Chord-Seed . You should see output similar to the below: Cloning into 'E-Chord-Seed'... remote: Enumerating objects: 48, done. remote: Counting objects: 100% (48/48), done. remote: Compressing objects: 100% (37/37), done. Receiving objects: 100% (48/48), 10.67 KiB | 214.00 KiB/s, done. remote: Total 48 (delta 24), reused 30 (delta 10), pack-reused 0 (from 0) Resolving deltas: 100% (24/24), done. If not, make sure git is installed on the system. Another way to get the repository is through GitHub's web interface itself. Above the repository's directory structure on the main page, you will find a button with the text <> Code . Click it, and choose the Download ZIP option. You should now have a ZIP file with the repository in it. Simply extract it at your location of choice. Once the repository has been cloned, open a terminal at its location. If you cloned the repository through the command line interface, using the command cd E-Chord-Seed directly after will suffice. Optionally, you can change the seed server parameters before starting the server. Open the params.json file. Here, you can change the server's port, which is 8000 by default. For a full list of parameters, refer to the parameter reference guide . Once finished, enter the command python3 server.py . If nothing goes wrong, you should see the below output: MainThread-INFO: Loaded params MainThread-INFO: Starting node on [your local IP]:[your PORT] You have now successfully set up the Seed Server!","title":"Seed Server"},{"location":"how-to-guides/setting-up/#nodes","text":"The next part is setting up the actual E-Chord network. On a computer you'd like to add to the network as a node, get the E-Chord repository, similarly to how you previously downloaded the seed repository. There is just one step that must be completed before entering the network. Open the config/params.json file. There, you'll find various parameters about the network that can be customized. For a full list of them, refer to the parameter reference guide . Under \"seed_server\" , you'll find the following parameters: \"seed_server\": { \"ip\": \"192.168.1.15\", \"port\": 8000, \"attempt_limit\": 4 } Make sure to change the seed server IP to the IP of the computer running the seed server on the local network. This was available at the [your local IP] part of the output after you ran the seed server. Similarly, change the port number to the number you used to run the seed server. Additionally, change the port number under host to a unique port number. Optionally, change any other parameters as needed. However, make sure that any changes are applied to every node in the network, since most of these parameters must be the same across all nodes. Again, refer to the guide for more information on individual parameters. Once the parameters are set up as required, you can launch the node on this system using the python3 start_node.py command. Do not provide the port as an argument! If you do this, the node will run on localhost and won't be visible to other computers in the network. If everything has been done correctly, you should see output confirming that the node has joined the network. If this is the first computer you've added to the network, the output should look somewhat like this: MainThread:11932-INFO: No other nodes in the network MainThread:11932-INFO: Initialized predecessor and sucessor list to self MainThread:11932-INFO: Starting node on [your local IP]:[your PORT] You'll need to repeat this process for each new computer you add to the network. As you add more computers, the output of each should start to indicate the presence of others in the network. Similarly, the output of the seed server should indicate awareness of each of the nodes joining the network.","title":"Nodes"},{"location":"reference/communication/","text":"Communication Communicating with an active E-Chord network is done through TCP sockets. A request-response format is used. For more information, read the communication guide . Below is a comprehensive list of requests and equivalent responses. The format used to indicate the correct data types for request and response data is as follows. The data being displayed consist of key-value pairs for the value of the body field of a request/response. For each request/response, the data columns in the tables below contain each key name (as a string) and its corresponding value type. If ANY is used as a value type, it means the key can contain any type of value. If square brackets surround a value, that means a JSON array is expected. This can contain multiple instances of whichever types of data are contained within it. Requests For a request to be valid, it must have a valid request type and contain the expected data in its body. Ordinary requests These are requests that are ordinarily used by a client outside the network. Request Type Expected Data Description find_key key : string Asks for the value for a key. find_and_store_key key : string value : ANY Asks a node to locate the correct node and store a pair. find_and_delete_key key : string Asks a node to locate the correct node and delete a pair by key. poll - Polls a node, asking if it is alive. Normally used by seed server, but may be used by other clients. Debugging requests These are requests that serve debugging purposes. It is not expected that these requests be used in ordinary function. Request Type Expected Data Description debug_leave_ring - Tells node to leave ring in a semi-ordinary fashion. This consists of moving existing data to another appropriate node, but not notifying nodes beyond this. debug_pred - Tells node to output its predecessor's address and node ID. debug_succ_list - Tells node to output its successor list. debug_finger_table - Tells node to output its finger table. debug_storage - Tells node to output its storage. This includes all pairs saved on the node. debug_fail - Tells node to abruptly fail. This consists of leaving the network without performing any actions. debug_get_total_keys - Tells node to return the number of keys stored on it. Internal requests These are requests used between nodes to ensure optimal network operation. These requests should almost never be used during ordinary function, since they could disrupt network operations. Use these at your own risk. Request Type Expected Data Description get_successor - Asks a node for its successor. get_predecessor - Asks a node for its predecessor. find_successor for_id : int Asks a node to find the correct successor for an ID. get_closest_preceding_finger for_key_id : int Asks a node for the closest preceding finger it has for an ID. get_prev_successor_list - Asks a node for its successor list. lookup key : string Asks a node to return the value for a key from its storage. delete_key key : string Asks a node to delete a pair by key from its storage. The rest of the internal requests, seen below, will almost certainly disrupt network operations, to some degree, if used by outside clients. Request Type Expected Data Description update_predecessor ip : string port : int node_id : int Tells a node to update its predecessor to a new node. clear_predecessor - Tells a node to clear its predecessor. store_key key : string value : ANY key_id : int Tells a node to store a new pair in its storage. batch_store_keys keys : [{ key : string value : ANY key_id : int }] Tells a node to store a set of pairs in its storage. Responses For most types of requests, there is an expected response. Few requests do not expect a response. This is denoted below by a missing response status. Responses always contain a status code in their header. Below are the response codes in use by E-Chord: Status Code Meaning STATUS_OK 200 Request served successfully. STATUS_NOT_FOUND 404 Unable to find resource. Responses might also contain data. In that case, this is contained in the response body. Note that, if the returned status is STATUS_NOT_FOUND, data will not be contained in the response body. Ordinary requests Request Type Response Status Response Data find_key STATUS_OK STATUS_NOT_FOUND value : ANY find_and_store_key STATUS_OK STATUS_NOT_FOUND - find_and_delete_key STATUS_OK STATUS_NOT_FOUND - poll STATUS_OK - Debugging requests Request Type Response Status Response Data debug_leave_ring STATUS_OK - debug_pred STATUS_OK - debug_succ_list STATUS_OK - debug_finger_table STATUS_OK - debug_storage STATUS_OK - debug_fail - - debug_get_total_keys STATUS_OK total_keys : int Internal requests Request Type Response Status Response Data get_successor STATUS_OK ip : string port : int node_id : int get_predecessor STATUS_OK STATUS_NOT_FOUND ip : string port : int node_id : int find_successor STATUS_OK STATUS_NOT_FOUND ip : string port : int node_id : int get_closest_preceding_finger STATUS_OK fingers : [{ ip : string port : int node_id : int }] contains_successor : bool get_prev_successor_list STATUS_OK successor_list : [{ ip : string port : int node_id : int }] lookup STATUS_OK STATUS_NOT_FOUND value : ANY delete_key STATUS_OK STATUS_NOT_FOUND - update_predecessor STATUS_OK - clear_predecessor STATUS_OK - store_key STATUS_OK - batch_store_keys STATUS_OK -","title":"Communication"},{"location":"reference/communication/#communication","text":"Communicating with an active E-Chord network is done through TCP sockets. A request-response format is used. For more information, read the communication guide . Below is a comprehensive list of requests and equivalent responses. The format used to indicate the correct data types for request and response data is as follows. The data being displayed consist of key-value pairs for the value of the body field of a request/response. For each request/response, the data columns in the tables below contain each key name (as a string) and its corresponding value type. If ANY is used as a value type, it means the key can contain any type of value. If square brackets surround a value, that means a JSON array is expected. This can contain multiple instances of whichever types of data are contained within it.","title":"Communication"},{"location":"reference/communication/#requests","text":"For a request to be valid, it must have a valid request type and contain the expected data in its body.","title":"Requests"},{"location":"reference/communication/#ordinary-requests","text":"These are requests that are ordinarily used by a client outside the network. Request Type Expected Data Description find_key key : string Asks for the value for a key. find_and_store_key key : string value : ANY Asks a node to locate the correct node and store a pair. find_and_delete_key key : string Asks a node to locate the correct node and delete a pair by key. poll - Polls a node, asking if it is alive. Normally used by seed server, but may be used by other clients.","title":"Ordinary requests"},{"location":"reference/communication/#debugging-requests","text":"These are requests that serve debugging purposes. It is not expected that these requests be used in ordinary function. Request Type Expected Data Description debug_leave_ring - Tells node to leave ring in a semi-ordinary fashion. This consists of moving existing data to another appropriate node, but not notifying nodes beyond this. debug_pred - Tells node to output its predecessor's address and node ID. debug_succ_list - Tells node to output its successor list. debug_finger_table - Tells node to output its finger table. debug_storage - Tells node to output its storage. This includes all pairs saved on the node. debug_fail - Tells node to abruptly fail. This consists of leaving the network without performing any actions. debug_get_total_keys - Tells node to return the number of keys stored on it.","title":"Debugging requests"},{"location":"reference/communication/#internal-requests","text":"These are requests used between nodes to ensure optimal network operation. These requests should almost never be used during ordinary function, since they could disrupt network operations. Use these at your own risk. Request Type Expected Data Description get_successor - Asks a node for its successor. get_predecessor - Asks a node for its predecessor. find_successor for_id : int Asks a node to find the correct successor for an ID. get_closest_preceding_finger for_key_id : int Asks a node for the closest preceding finger it has for an ID. get_prev_successor_list - Asks a node for its successor list. lookup key : string Asks a node to return the value for a key from its storage. delete_key key : string Asks a node to delete a pair by key from its storage. The rest of the internal requests, seen below, will almost certainly disrupt network operations, to some degree, if used by outside clients. Request Type Expected Data Description update_predecessor ip : string port : int node_id : int Tells a node to update its predecessor to a new node. clear_predecessor - Tells a node to clear its predecessor. store_key key : string value : ANY key_id : int Tells a node to store a new pair in its storage. batch_store_keys keys : [{ key : string value : ANY key_id : int }] Tells a node to store a set of pairs in its storage.","title":"Internal requests"},{"location":"reference/communication/#responses","text":"For most types of requests, there is an expected response. Few requests do not expect a response. This is denoted below by a missing response status. Responses always contain a status code in their header. Below are the response codes in use by E-Chord: Status Code Meaning STATUS_OK 200 Request served successfully. STATUS_NOT_FOUND 404 Unable to find resource. Responses might also contain data. In that case, this is contained in the response body. Note that, if the returned status is STATUS_NOT_FOUND, data will not be contained in the response body.","title":"Responses"},{"location":"reference/communication/#ordinary-requests_1","text":"Request Type Response Status Response Data find_key STATUS_OK STATUS_NOT_FOUND value : ANY find_and_store_key STATUS_OK STATUS_NOT_FOUND - find_and_delete_key STATUS_OK STATUS_NOT_FOUND - poll STATUS_OK -","title":"Ordinary requests"},{"location":"reference/communication/#debugging-requests_1","text":"Request Type Response Status Response Data debug_leave_ring STATUS_OK - debug_pred STATUS_OK - debug_succ_list STATUS_OK - debug_finger_table STATUS_OK - debug_storage STATUS_OK - debug_fail - - debug_get_total_keys STATUS_OK total_keys : int","title":"Debugging requests"},{"location":"reference/communication/#internal-requests_1","text":"Request Type Response Status Response Data get_successor STATUS_OK ip : string port : int node_id : int get_predecessor STATUS_OK STATUS_NOT_FOUND ip : string port : int node_id : int find_successor STATUS_OK STATUS_NOT_FOUND ip : string port : int node_id : int get_closest_preceding_finger STATUS_OK fingers : [{ ip : string port : int node_id : int }] contains_successor : bool get_prev_successor_list STATUS_OK successor_list : [{ ip : string port : int node_id : int }] lookup STATUS_OK STATUS_NOT_FOUND value : ANY delete_key STATUS_OK STATUS_NOT_FOUND - update_predecessor STATUS_OK - clear_predecessor STATUS_OK - store_key STATUS_OK - batch_store_keys STATUS_OK -","title":"Internal requests"},{"location":"reference/parameters/","text":"Parameters E-Chord allows for customization of the network through its parameters. Note that, unless otherwise stated, the parameters for every node in a network must be the same. Executing a network whose nodes use different values for their parameters will result in undefined behavior. Node parameters All configurable parameters used by an E-Chord node can be found in the config/params.json file. In the below tables, a list of all available node parameters is presented, along with its function. seed_server These are parameters related to the seed server. Parameter Value Type Description ip string IP address of the seed server. port int Port of the seed server. attempt_limit int Number of times to attempt connection before exiting. host These are parameters related to the server hosted by the node. These parameters may be different between nodes. Parameter Value Type Description server_port int Port number to use when starting the node. ring These are parameters related to the virtual ring the network uses. Parameter Value Type Description bits int Number of bits of IDs on the ring. This also determines the maximum number of nodes that can join the ring, which would be 2 bits . fallback_fingers int Number of additional fingers to send with the closest preceding finger as a response to a get_closest_preceding_finger RPC. These are used as fallback fingers in case the closest is down. stabilize_delay int Seconds between each stabilize event. net These are parameters related to networking operations. Parameter Value Type Description timeout int Seconds to wait for a response before timeout. connection_lifespan int Seconds to wait before closing a connection that hasn't been used. This resets whenever a connection is used. data_size int Maximum size (in bytes) of a message. Messages longer than this are sent in chunks. logging These are parameters related to the output produced by each node. The value of these parameters can be different between nodes in a network. Parameter Value Type Description level string Level of logging to display. Available levels are INFO , DEBUG and CRITICAL . testing These are parameters related to the simulation scripts and do not affect node execution. Parameter Value Type Description initial_port int Port of the first node started by the mass_node_join script. Subsequent nodes will use increasing port numbers, beginning with this value. total_nodes int Total number of nodes to attempt to add to the simulated network. stabilize_wait_period - Unused percentage_to_remove int Percentage of total nodes to remove from the network, used by the mass_node_leave script. Seed server parameters If you're using the default seed server provided with E-Chord, there are a few parameters you can customize. These can be found in the params.json file. host These are parameters related to the server hosted by the seed server. Parameter Value Type Description server_port int Port to use when starting the seed server. net These are parameters related to networking operations. Parameter Value Type Description data_size int Maximum size (in bytes) of a message. Messages longer than this are sent in chunks. This should be the same as the data_size parameter of the nodes. timeout int Seconds to wait for a response before timeout. polling_delay int Seconds to wait between polling events. logging These are parameters related to the output produced by the seed server. Parameter Value Type Description level string Level of logging to display. Available levels are INFO , DEBUG and CRITICAL .","title":"Parameters"},{"location":"reference/parameters/#parameters","text":"E-Chord allows for customization of the network through its parameters. Note that, unless otherwise stated, the parameters for every node in a network must be the same. Executing a network whose nodes use different values for their parameters will result in undefined behavior.","title":"Parameters"},{"location":"reference/parameters/#node-parameters","text":"All configurable parameters used by an E-Chord node can be found in the config/params.json file. In the below tables, a list of all available node parameters is presented, along with its function.","title":"Node parameters"},{"location":"reference/parameters/#seed_server","text":"These are parameters related to the seed server. Parameter Value Type Description ip string IP address of the seed server. port int Port of the seed server. attempt_limit int Number of times to attempt connection before exiting.","title":"seed_server"},{"location":"reference/parameters/#host","text":"These are parameters related to the server hosted by the node. These parameters may be different between nodes. Parameter Value Type Description server_port int Port number to use when starting the node.","title":"host"},{"location":"reference/parameters/#ring","text":"These are parameters related to the virtual ring the network uses. Parameter Value Type Description bits int Number of bits of IDs on the ring. This also determines the maximum number of nodes that can join the ring, which would be 2 bits . fallback_fingers int Number of additional fingers to send with the closest preceding finger as a response to a get_closest_preceding_finger RPC. These are used as fallback fingers in case the closest is down. stabilize_delay int Seconds between each stabilize event.","title":"ring"},{"location":"reference/parameters/#net","text":"These are parameters related to networking operations. Parameter Value Type Description timeout int Seconds to wait for a response before timeout. connection_lifespan int Seconds to wait before closing a connection that hasn't been used. This resets whenever a connection is used. data_size int Maximum size (in bytes) of a message. Messages longer than this are sent in chunks.","title":"net"},{"location":"reference/parameters/#logging","text":"These are parameters related to the output produced by each node. The value of these parameters can be different between nodes in a network. Parameter Value Type Description level string Level of logging to display. Available levels are INFO , DEBUG and CRITICAL .","title":"logging"},{"location":"reference/parameters/#testing","text":"These are parameters related to the simulation scripts and do not affect node execution. Parameter Value Type Description initial_port int Port of the first node started by the mass_node_join script. Subsequent nodes will use increasing port numbers, beginning with this value. total_nodes int Total number of nodes to attempt to add to the simulated network. stabilize_wait_period - Unused percentage_to_remove int Percentage of total nodes to remove from the network, used by the mass_node_leave script.","title":"testing"},{"location":"reference/parameters/#seed-server-parameters","text":"If you're using the default seed server provided with E-Chord, there are a few parameters you can customize. These can be found in the params.json file.","title":"Seed server parameters"},{"location":"reference/parameters/#host_1","text":"These are parameters related to the server hosted by the seed server. Parameter Value Type Description server_port int Port to use when starting the seed server.","title":"host"},{"location":"reference/parameters/#net_1","text":"These are parameters related to networking operations. Parameter Value Type Description data_size int Maximum size (in bytes) of a message. Messages longer than this are sent in chunks. This should be the same as the data_size parameter of the nodes. timeout int Seconds to wait for a response before timeout. polling_delay int Seconds to wait between polling events.","title":"net"},{"location":"reference/parameters/#logging_1","text":"These are parameters related to the output produced by the seed server. Parameter Value Type Description level string Level of logging to display. Available levels are INFO , DEBUG and CRITICAL .","title":"logging"},{"location":"reference/simulation/","text":"Local simulation E-Chord can be simulated locally, on one machine, using the provided simulation scripts. Various parameters of the simulation can be customized, as described in the parameter reference guide . These scripts should be used after a seed server is already running locally. All scripts should be run from the scripts directory. Adding nodes To add nodes to the network, use the mass_node_join script. Sample usage: python3 mass_node_join.py Inserting and looking up data To insert or look up data, use the mass_data script. Sample usage: python3 mass_data.py [filedir] [i/l] (limit) (delay) Required arguments include: filedir : Relative or absolute path of JSON file containing data. i/l : i for insertion, or l for lookup. Note that the data file must have a specific format. It should contain a single JSON object with multiple keys, each of which will be inserted into the network with whichever value it contains, as a pair. Optional arguments include: limit : Upper limit for number of keys to insert to the network. delay : Seconds to wait before starting data insertion. Causing node failure To cause a mass concurrent node failure, use the mass_node_leave script. Sample usage: python3 mass_node_leave.py A percentage of existing nodes, based on a parameter, will be removed from the network. Note that those nodes will attempt to move their data to other nodes before leaving. Visualizing the network To visualize the network, its current stability level and the keys contained within, use the visualizer script. Sample usage: python3 visualizer.py","title":"Local simulation"},{"location":"reference/simulation/#local-simulation","text":"E-Chord can be simulated locally, on one machine, using the provided simulation scripts. Various parameters of the simulation can be customized, as described in the parameter reference guide . These scripts should be used after a seed server is already running locally. All scripts should be run from the scripts directory.","title":"Local simulation"},{"location":"reference/simulation/#adding-nodes","text":"To add nodes to the network, use the mass_node_join script. Sample usage: python3 mass_node_join.py","title":"Adding nodes"},{"location":"reference/simulation/#inserting-and-looking-up-data","text":"To insert or look up data, use the mass_data script. Sample usage: python3 mass_data.py [filedir] [i/l] (limit) (delay) Required arguments include: filedir : Relative or absolute path of JSON file containing data. i/l : i for insertion, or l for lookup. Note that the data file must have a specific format. It should contain a single JSON object with multiple keys, each of which will be inserted into the network with whichever value it contains, as a pair. Optional arguments include: limit : Upper limit for number of keys to insert to the network. delay : Seconds to wait before starting data insertion.","title":"Inserting and looking up data"},{"location":"reference/simulation/#causing-node-failure","text":"To cause a mass concurrent node failure, use the mass_node_leave script. Sample usage: python3 mass_node_leave.py A percentage of existing nodes, based on a parameter, will be removed from the network. Note that those nodes will attempt to move their data to other nodes before leaving.","title":"Causing node failure"},{"location":"reference/simulation/#visualizing-the-network","text":"To visualize the network, its current stability level and the keys contained within, use the visualizer script. Sample usage: python3 visualizer.py","title":"Visualizing the network"},{"location":"tutorials/adding-data/","text":"Adding data to an E-Chord network using the client interface In this tutorial, we will be adding data to an existing E-Chord network using a basic interface. For our purposes, we will be doing this with a network running locally on a single computer. We will be using the network created in the Setting up a three-node network on a single computer tutorial. Executing the visualizer To begin with, execute the visualizer script at scripts/visualizer.py if you haven't already. This will show us a view of the current network state, and will update every few seconds to account for changes. Currently, the output of the script is the following: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 0 Stability: 100.00% Pay attention to the third column of the table. The values in each row of that column inform us how many keys (i.e pieces of data) each node holds. As we still have not inserted any data into the network, none of the existing nodes store any keys. Adding data using the client script To actually store some data in our network, we must first know how to communicate with it. For this, it is necessary that we know the address of some node in the network. In our example, we inserted all three nodes by hand, so we know the addresses for each of them. Since we passed the port numbers as arguments when running nodes, the IP address defaults to the empty IP address, which is equivalent to localhost . The port number used for the first node was 9150. Using these, we can now execute the client.py script, located in the main repository. We use the command python3 client.py localhost 9150 . This gives us access to a simple interface, which allows for basic communication with the node. By typing in help , we get the following output: Available commands: >lookup [key] >insert [key] [value] >delete [key] >leave Remember that the Chord protocol describes a DHT (distributed hash table). As such, we store data in key-value pairs, similar to a regular hash table. Let's insert our first pair. We'd like to insert the string \"Hello\" as the key, with the string \"World\" as a value. To do this, we execute the insert Hello World command in the client interface. As long as the network is running and the node address we gave is valid, we should see the output Successfully stored pair . Observing the visualizer Let's now turn our attention back to the visualizer script. Return to the terminal window where you ran the script, and you should notice some changes: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 1 Stability: 100.00% The visualizer informs us that a key has indeed been added to a node with ID 2503. You might think that this is the node we contacted using its address, but there is no guarantee that this is the case! In fact, contacting a node is equivalent to contacting the entire network: we add data to and read from the entire network, not just the singular node. Adding more data To make this clearer, let's add more data to the network. Similarly to before, execute the insert [key] [value] command in the client interface a few times. Following this, observe the visualizer output again: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 9 Stability: 100.00% Notice how not all of the keys have been stored in the same node and, instead, they've been spread to the three existing nodes. It might seem like too many have been given to the node with ID 15626, but the reason as to why that happens is connected to how the protocol works and our parameters. For more information on this, visit the Chord protocol discussion page. Reading stored data Of course, being able to add data is useless without being able to also retrieve it. As you might have noticed in the available commands before, we can also read any of the data we store in E-Chord. To make this even more interesting, however, let's try reading some of that data from a different node, instead of the one we contacted to store it. After all, that's how we'll know the network isn't just cheating and storing everything in one node! Run the client script again, this time with the address of another node. In our example, we used port 9152 for the third node, so let's contact that one, using the command python3 client.py localhost 9152 . We'll now try to read the first piece of data we stored. The key was Hello , and the expected value stored with it is World . Use the command lookup Hello in the client interface. You should notice the below output: >lookup Hello Key hello has value: world As you can see, the network is able to locate the value corresponding to that key, even though we inserted it at a different node. In fact, the node we contact is only a \"representative\" of the network. There is no guarantee that anything we tell it to store will be stored at that node, nor that anything we read is necessarily stored at that node. Don't worry about the fact that World is all lowercase. This isn't an issue with the network, but rather the client itself, which lowers the input we provide. In a real execution, this would not be an issue. Play around with storing and reading data! Even though the client script only provides limited capabilities, you can still use the visualizer to see that anything you store is placed at a node determined by the network and is readable from every other node. Deleting data Of course, hash tables also provide the user with the ability to delete existing data. E-Chord is no different, in that any data previously stored can be deleted. In this case, using the client interface, we can try deleting the first key we inserted using the delete Hello command. >delete Hello Successfully deleted key. The output informs us that the previously inserted key was indeed deleted. Moving over to the visualizer tab, we can see that one key is indeed missing (we had 9 before): \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 8 Stability: 100.00% Finally, looking up the key confirms that it no longer exists in the network. >lookup Hello Key not found. Conclusion You have now successfully used the client interface to insert, read and delete data from an E-Chord network. Later on, we will also dig deeper into the underlying mechanisms for communicating with the network, allowing for more freedom in how we store and retrieve data.","title":"Adding data to an E-Chord network using the client interface"},{"location":"tutorials/adding-data/#adding-data-to-an-e-chord-network-using-the-client-interface","text":"In this tutorial, we will be adding data to an existing E-Chord network using a basic interface. For our purposes, we will be doing this with a network running locally on a single computer. We will be using the network created in the Setting up a three-node network on a single computer tutorial.","title":"Adding data to an E-Chord network using the client interface"},{"location":"tutorials/adding-data/#executing-the-visualizer","text":"To begin with, execute the visualizer script at scripts/visualizer.py if you haven't already. This will show us a view of the current network state, and will update every few seconds to account for changes. Currently, the output of the script is the following: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 0 Stability: 100.00% Pay attention to the third column of the table. The values in each row of that column inform us how many keys (i.e pieces of data) each node holds. As we still have not inserted any data into the network, none of the existing nodes store any keys.","title":"Executing the visualizer"},{"location":"tutorials/adding-data/#adding-data-using-the-client-script","text":"To actually store some data in our network, we must first know how to communicate with it. For this, it is necessary that we know the address of some node in the network. In our example, we inserted all three nodes by hand, so we know the addresses for each of them. Since we passed the port numbers as arguments when running nodes, the IP address defaults to the empty IP address, which is equivalent to localhost . The port number used for the first node was 9150. Using these, we can now execute the client.py script, located in the main repository. We use the command python3 client.py localhost 9150 . This gives us access to a simple interface, which allows for basic communication with the node. By typing in help , we get the following output: Available commands: >lookup [key] >insert [key] [value] >delete [key] >leave Remember that the Chord protocol describes a DHT (distributed hash table). As such, we store data in key-value pairs, similar to a regular hash table. Let's insert our first pair. We'd like to insert the string \"Hello\" as the key, with the string \"World\" as a value. To do this, we execute the insert Hello World command in the client interface. As long as the network is running and the node address we gave is valid, we should see the output Successfully stored pair .","title":"Adding data using the client script"},{"location":"tutorials/adding-data/#observing-the-visualizer","text":"Let's now turn our attention back to the visualizer script. Return to the terminal window where you ran the script, and you should notice some changes: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 1 Stability: 100.00% The visualizer informs us that a key has indeed been added to a node with ID 2503. You might think that this is the node we contacted using its address, but there is no guarantee that this is the case! In fact, contacting a node is equivalent to contacting the entire network: we add data to and read from the entire network, not just the singular node.","title":"Observing the visualizer"},{"location":"tutorials/adding-data/#adding-more-data","text":"To make this clearer, let's add more data to the network. Similarly to before, execute the insert [key] [value] command in the client interface a few times. Following this, observe the visualizer output again: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 9 Stability: 100.00% Notice how not all of the keys have been stored in the same node and, instead, they've been spread to the three existing nodes. It might seem like too many have been given to the node with ID 15626, but the reason as to why that happens is connected to how the protocol works and our parameters. For more information on this, visit the Chord protocol discussion page.","title":"Adding more data"},{"location":"tutorials/adding-data/#reading-stored-data","text":"Of course, being able to add data is useless without being able to also retrieve it. As you might have noticed in the available commands before, we can also read any of the data we store in E-Chord. To make this even more interesting, however, let's try reading some of that data from a different node, instead of the one we contacted to store it. After all, that's how we'll know the network isn't just cheating and storing everything in one node! Run the client script again, this time with the address of another node. In our example, we used port 9152 for the third node, so let's contact that one, using the command python3 client.py localhost 9152 . We'll now try to read the first piece of data we stored. The key was Hello , and the expected value stored with it is World . Use the command lookup Hello in the client interface. You should notice the below output: >lookup Hello Key hello has value: world As you can see, the network is able to locate the value corresponding to that key, even though we inserted it at a different node. In fact, the node we contact is only a \"representative\" of the network. There is no guarantee that anything we tell it to store will be stored at that node, nor that anything we read is necessarily stored at that node. Don't worry about the fact that World is all lowercase. This isn't an issue with the network, but rather the client itself, which lowers the input we provide. In a real execution, this would not be an issue. Play around with storing and reading data! Even though the client script only provides limited capabilities, you can still use the visualizer to see that anything you store is placed at a node determined by the network and is readable from every other node.","title":"Reading stored data"},{"location":"tutorials/adding-data/#deleting-data","text":"Of course, hash tables also provide the user with the ability to delete existing data. E-Chord is no different, in that any data previously stored can be deleted. In this case, using the client interface, we can try deleting the first key we inserted using the delete Hello command. >delete Hello Successfully deleted key. The output informs us that the previously inserted key was indeed deleted. Moving over to the visualizer tab, we can see that one key is indeed missing (we had 9 before): \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 1 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 7 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 8 Stability: 100.00% Finally, looking up the key confirms that it no longer exists in the network. >lookup Hello Key not found.","title":"Deleting data"},{"location":"tutorials/adding-data/#conclusion","text":"You have now successfully used the client interface to insert, read and delete data from an E-Chord network. Later on, we will also dig deeper into the underlying mechanisms for communicating with the network, allowing for more freedom in how we store and retrieve data.","title":"Conclusion"},{"location":"tutorials/basic-network/","text":"Setting up a three node E-Chord network on a single computer In this tutorial, we will set up an E-Chord network consisting of three nodes. The entire network, including the seed server, will run on a single computer, only using the local network for communication. To do this, we'll need both the repository for the seed server and for the node. Start by cloning both of these repositories at a location of your choosing. For more information on how to do this, refer to the setting up guide . Starting the seed server As always, the first step is to start the seed server. Open a terminal at the seed server's location and use the python3 server.py command. The output should look something like this: MainThread-INFO: Loaded params MainThread-INFO: Starting node on 192.168.1.12:8000 If instead, you get a Permission denied error, it most likely means the port is already in use. In that case, edit the port number in params.json and try again. You have now started the seed server! The server will wait for new nodes to contact it, so the next step is adding some nodes to the network. Adding a node Go to the location where you cloned the main repository. A tree view of the repository should look like this: . \u251c\u2500\u2500 client.py \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 params.json \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 mass_data.py \u2502 \u251c\u2500\u2500 mass_node_join.py \u2502 \u251c\u2500\u2500 mass_node_leave.py \u2502 \u2514\u2500\u2500 visualizer.py \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 ConnectionPool.py \u2502 \u251c\u2500\u2500 Finger.py \u2502 \u251c\u2500\u2500 Node.py \u2502 \u251c\u2500\u2500 rpc_handlers.py \u2502 \u251c\u2500\u2500 Storage.py \u2502 \u2514\u2500\u2500 utils.py \u2514\u2500\u2500 start_node.py 3 directories, 15 files If this looks like a lot, don't worry! We won't be using most of these for our purposes. In fact, there is only one step we need to perform before adding our first node to the network: setting up the seed server parameters. This is essentially equivalent to telling our nodes how to contact the seed server we just opened. To do this, open the config/params.json file. Find the \"seed_server\" section and replace the default IP (192.168.1.15) with the seed server's IP, as well as the default port (8000) with the seed server's port. Make sure to save these changes. Once you've done this, return to the terminal at the repository's location and run the start_node.py file using the python3 start_node.py 9150 command. If you get the permission denied error again, you'll need to run using a different port number. Simply change the last argument of the command. For instance, if you want to run using port number 9200, run the command python3 start_node.py 9200 . If all goes well, you should see output similar to the following: MainThread:15626-INFO: No other nodes in the network MainThread:15626-INFO: Initialized predecessor and sucessor list to self MainThread:15626-INFO: Starting node on :9150 This means our node has joined the network successfully! Since this is the first node to join, we get the No other nodes in the network message. You might also notice that as time goes on, more and more messages fill the output. This is normal! The Chord protocol contains various periodic routines, so these messages simply inform us every time one of those completes. Interactions with the seed server If you return to the terminal window where the seed server is running, you'll notice there is new output there as well. When we initially started our node, we got output such as this: Thread-3 (handle_connection)-INFO: Got request for seed, sending None MainThread-INFO: Added ('192.168.1.12', 9150, 11932) This tells us the node we started contacted the seed server and asked for a seed, but got nothing (because there are no nodes in the network yet)! The node was then added to the seed server's list. Afterwards, you'll notice the seed server periodically outputting messages like these: Poll-INFO: Polling node: ('192.168.1.12', 9150) Poll-INFO: Node is alive This means our seed server is periodically contacting our node, asking it if it is still there. The node responds and so the server determines it is alive, therefore keeping it in its list. Adding a second node Let's now add a second node. Just like before, open a new terminal window at the location of the node repository and execute the command python3 start_node.py 9951 . You don't necessarily have to use the same port number, but make sure it is different to the ones used by the seed server and your first node. Since you're executing this in the same repository, you've already made the necessary changes to the parameters, so you don't need to worry about making those changes again. You should now notice that the initial messages we get as output are different to those of the first node. MainThread:2503-INFO: Got seed from seed server MainThread:2503-INFO: Asking seed for successor MainThread:2503-INFO: Got successor MainThread:2503-INFO: Asking successor for this node's successor list MainThread:2503-INFO: Initialized successor list MainThread:2503-INFO: Starting node on :9151 These tell us that the second node recognizes that there's another node in the network and contacts it. We can see the start of this exchange in the seed server's output as well. Thread-172 (handle_connection)-INFO: Got request for seed, sending ('192.168.1.12', 9150, 11932) MainThread-INFO: Added ('', 9151, 2503) The second node asked the seed server for a seed. This time, since the first node was already in the network, the seed server provided the second node with the information necessary to connect to it. Between the stabilization messages of the first node, you'll find the below output as well: MainThread:11932-INFO: Got new successor This tells us that the first node is now also aware of the second node's existence. The network should now contain both of the nodes we added. Finally, let's add a third node and observe the output we get. Just like before, open a new terminal and start another node, with a different port to those already used. The result should be the same as when we added the second node. Both existing nodes connect to the new node. The seed server will initially send information about one of the two existing nodes randomly. If you repeat this experiment multiple times, you'll find that sometimes, the third node initially gets information about the first node, whereas other times it gets information about the second one. Finally, we can use one of the provided scripts to get a better view of the current network. Open a new terminal at the repository's location. Use cd scripts to enter the scripts directory and execute the python3 visualizer.py command. You'll notice output similar to this: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 0 Stability: 100.00% If the nodes aren't visible, make sure you're using ports in the visualizer's range. By default, this range is 9150 to 9250. You are now able to confirm that everything is indeed stable. Each node should have the next one as its successor, increasing in ID, until the last one, which connects to the first one again. The predecessors, of course, go in the opposite direction. Conclusion If this is the case, then you have successfully created an E-Chord network on your computer! Note that even though this network runs in the exact same way as a regular network, it all happens locally on a single machine. For more information on the differences between this and an actual network, refer to the E-Chord discussion .","title":"Setting up a three node E-Chord network on a single computer"},{"location":"tutorials/basic-network/#setting-up-a-three-node-e-chord-network-on-a-single-computer","text":"In this tutorial, we will set up an E-Chord network consisting of three nodes. The entire network, including the seed server, will run on a single computer, only using the local network for communication. To do this, we'll need both the repository for the seed server and for the node. Start by cloning both of these repositories at a location of your choosing. For more information on how to do this, refer to the setting up guide .","title":"Setting up a three node E-Chord network on a single computer"},{"location":"tutorials/basic-network/#starting-the-seed-server","text":"As always, the first step is to start the seed server. Open a terminal at the seed server's location and use the python3 server.py command. The output should look something like this: MainThread-INFO: Loaded params MainThread-INFO: Starting node on 192.168.1.12:8000 If instead, you get a Permission denied error, it most likely means the port is already in use. In that case, edit the port number in params.json and try again. You have now started the seed server! The server will wait for new nodes to contact it, so the next step is adding some nodes to the network.","title":"Starting the seed server"},{"location":"tutorials/basic-network/#adding-a-node","text":"Go to the location where you cloned the main repository. A tree view of the repository should look like this: . \u251c\u2500\u2500 client.py \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 params.json \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 mass_data.py \u2502 \u251c\u2500\u2500 mass_node_join.py \u2502 \u251c\u2500\u2500 mass_node_leave.py \u2502 \u2514\u2500\u2500 visualizer.py \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 ConnectionPool.py \u2502 \u251c\u2500\u2500 Finger.py \u2502 \u251c\u2500\u2500 Node.py \u2502 \u251c\u2500\u2500 rpc_handlers.py \u2502 \u251c\u2500\u2500 Storage.py \u2502 \u2514\u2500\u2500 utils.py \u2514\u2500\u2500 start_node.py 3 directories, 15 files If this looks like a lot, don't worry! We won't be using most of these for our purposes. In fact, there is only one step we need to perform before adding our first node to the network: setting up the seed server parameters. This is essentially equivalent to telling our nodes how to contact the seed server we just opened. To do this, open the config/params.json file. Find the \"seed_server\" section and replace the default IP (192.168.1.15) with the seed server's IP, as well as the default port (8000) with the seed server's port. Make sure to save these changes. Once you've done this, return to the terminal at the repository's location and run the start_node.py file using the python3 start_node.py 9150 command. If you get the permission denied error again, you'll need to run using a different port number. Simply change the last argument of the command. For instance, if you want to run using port number 9200, run the command python3 start_node.py 9200 . If all goes well, you should see output similar to the following: MainThread:15626-INFO: No other nodes in the network MainThread:15626-INFO: Initialized predecessor and sucessor list to self MainThread:15626-INFO: Starting node on :9150 This means our node has joined the network successfully! Since this is the first node to join, we get the No other nodes in the network message. You might also notice that as time goes on, more and more messages fill the output. This is normal! The Chord protocol contains various periodic routines, so these messages simply inform us every time one of those completes.","title":"Adding a node"},{"location":"tutorials/basic-network/#interactions-with-the-seed-server","text":"If you return to the terminal window where the seed server is running, you'll notice there is new output there as well. When we initially started our node, we got output such as this: Thread-3 (handle_connection)-INFO: Got request for seed, sending None MainThread-INFO: Added ('192.168.1.12', 9150, 11932) This tells us the node we started contacted the seed server and asked for a seed, but got nothing (because there are no nodes in the network yet)! The node was then added to the seed server's list. Afterwards, you'll notice the seed server periodically outputting messages like these: Poll-INFO: Polling node: ('192.168.1.12', 9150) Poll-INFO: Node is alive This means our seed server is periodically contacting our node, asking it if it is still there. The node responds and so the server determines it is alive, therefore keeping it in its list.","title":"Interactions with the seed server"},{"location":"tutorials/basic-network/#adding-a-second-node","text":"Let's now add a second node. Just like before, open a new terminal window at the location of the node repository and execute the command python3 start_node.py 9951 . You don't necessarily have to use the same port number, but make sure it is different to the ones used by the seed server and your first node. Since you're executing this in the same repository, you've already made the necessary changes to the parameters, so you don't need to worry about making those changes again. You should now notice that the initial messages we get as output are different to those of the first node. MainThread:2503-INFO: Got seed from seed server MainThread:2503-INFO: Asking seed for successor MainThread:2503-INFO: Got successor MainThread:2503-INFO: Asking successor for this node's successor list MainThread:2503-INFO: Initialized successor list MainThread:2503-INFO: Starting node on :9151 These tell us that the second node recognizes that there's another node in the network and contacts it. We can see the start of this exchange in the seed server's output as well. Thread-172 (handle_connection)-INFO: Got request for seed, sending ('192.168.1.12', 9150, 11932) MainThread-INFO: Added ('', 9151, 2503) The second node asked the seed server for a seed. This time, since the first node was already in the network, the seed server provided the second node with the information necessary to connect to it. Between the stabilization messages of the first node, you'll find the below output as well: MainThread:11932-INFO: Got new successor This tells us that the first node is now also aware of the second node's existence. The network should now contain both of the nodes we added. Finally, let's add a third node and observe the output we get. Just like before, open a new terminal and start another node, with a different port to those already used. The result should be the same as when we added the second node. Both existing nodes connect to the new node. The seed server will initially send information about one of the two existing nodes randomly. If you repeat this experiment multiple times, you'll find that sometimes, the third node initially gets information about the first node, whereas other times it gets information about the second one. Finally, we can use one of the provided scripts to get a better view of the current network. Open a new terminal at the repository's location. Use cd scripts to enter the scripts directory and execute the python3 visualizer.py command. You'll notice output similar to this: \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2503 -> 8230 \u2551 15626 <- 8230 \u2551 2503: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 8230 -> 15626 \u2551 8230 <- 2503 \u2551 8230: 0 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 15626 -> 2503 \u2551 2503 <- 15626 \u2551 15626: 0 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Total keys: 0 Stability: 100.00% If the nodes aren't visible, make sure you're using ports in the visualizer's range. By default, this range is 9150 to 9250. You are now able to confirm that everything is indeed stable. Each node should have the next one as its successor, increasing in ID, until the last one, which connects to the first one again. The predecessors, of course, go in the opposite direction.","title":"Adding a second node"},{"location":"tutorials/basic-network/#conclusion","text":"If this is the case, then you have successfully created an E-Chord network on your computer! Note that even though this network runs in the exact same way as a regular network, it all happens locally on a single machine. For more information on the differences between this and an actual network, refer to the E-Chord discussion .","title":"Conclusion"},{"location":"tutorials/communication/","text":"Communicating with an active E-Chord network through separate software In this tutorial, we will create software that communicates with an active E-Chord network and uses it to store and retrieve data. We'll assume that you already have an E-Chord network up and running. This can either be a network simulation (running on one machine), or an actual E-Chord network. If you haven't done this yet, consider taking a look at the previous tutorials. A basic program using a hash table Since E-Chord is a distributed hash table, we'll first write a program that uses a regular hash table. After this, we'll replace the hash table with E-Chord, and show that the functionality is the same. We'll write the program in Python, where the equivalent of a hash table is a dictionary. The program will implement a simple login system. Users will be able to register using a username and password, and login to the system. Obviously, we won't focus on any sort of security for this. We also won't implement any form of defensive programming. This will only serve to demonstrate the function of E-Chord as a distributed hash table. users = {} while True: print(\"Welcome to Login System!\") c = input(\"Would you like to login or register (l/r)? \") if c == \"l\": username = input(\"Username: \") password = input(\"Password: \") if username not in users or users[username] != password: print(\"Incorrect credentials!\") continue print(f\"Welcome, {username}!\") elif c == \"r\": username = input(\"Username: \") password = input(\"Password: \") if username in users: print(\"Username taken!\") continue users[username] = password print(\"Successfully registered!\") The above program uses a hash table to store usernames, with the value corresponding to their password. Whenever a new user registers, the hash table is checked for the existence of the username. If it doesn't exist, it is added with the provided password as a value. When a user tries to log in, the hash table is checked for the existence of the username. If it exists and the value is the same as the provided password, login is successful. Below is a sample execution of the program: Welcome to Login System! Would you like to login or register (l/r)? r Username: Emily Password: ilovecats2 Successfully registered! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovewats2 Incorrect credentials! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovecats2 Welcome, Emily! Communication with the E-Chord network When communicating with an E-Chord network, we essentially need to talk to one of the nodes of the network. As such, we need to know the address of some node. If you've started the network yourself, you should know the addresses of the nodes within it, so use one of them. If you've used the scripts to create the network, open the config/params.json file. Under testing , you'll find the parameter initial_port . Its value is the port of the first node that was started by the script, incremented by 1 for each new node. As such, one possible address you can use is localhost:[initial_port] . If that one doesn't work, increment the port value by 1 and try again. Another way to find an active node in the network is simply by observing the output of the seed server. The seed server regularly polls nodes to confirm they're still there, so looking at its output, you'll find the addresses of recently polled nodes. Remember that an empty IP address string is equivalent to localhost . E-Chord nodes communicate using TCP sockets. Additionally, messages between nodes are all in JSON, and follow a specific format. For an in-depth explanation on how to structure your messages when communicating with the network, refer to this guide and the equivalent reference guide . In order to simplify this process here, we will use an existing function. This function, called ask_node , connects to the node with the address we provide and makes a request of a certain type. import socket import json def ask_node(peer_addr, req_type, body_dict, custom_timeout=10): \"\"\" Sends a request and returns the response :param peer_addr: (IP, port) of peer :param req_type: type of request for request header :param body_dict: dictionary of body :param custom_timeout: timeout for request :return: string response of peer \"\"\" request_dict = {\"header\": {\"type\": req_type}, \"body\": body_dict} request_msg = json.dumps(request_dict, indent=2) with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client: client.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) client.settimeout(custom_timeout) try: client.connect(peer_addr) client.sendall(request_msg.encode()) data = client.recv(8192).decode() except (socket.error, socket.timeout): return None if not data: return None return json.loads(data) There are various request types that can be made. As mentioned before, a full list can be viewed in the corresponding guides. As we move forward, we'll translate the dictionary (i.e hash map) operations into the appropriate requests for the E-Chord network. Points of interest in the hash table program Let's return to the Login System program from before and identify the points where dictionary operations are used. We'll then discuss how these operations can be translated into their equivalents for E-Chord. Firstly, at the very top, we create an empty dictionary. The next time the dictionary is used is when a user tries to login. if username not in users or users[username] != password: In fact, there are two dictionary operations here. The first one is username not in users , which checks for the existence of a key in the dictionary, while the second is users[username] , which retrieves a value from the dictionary by key. Finally, we use the dictionary twice when a user registers. Firstly, like before, we check for the existence of a username: if username not in users: If a username doesn't exist, we register the user with the credentials given: users[username] = password In total, we have four points where we perform dictionary operations. Two of those consist of checking for the existence of a key, one of getting the value for a key and one of inserting a new pair. Equivalent E-Chord operations To perform these operations in the E-Chord network, we'll have to make the appropriate requests to a node in the network. We have three types of operations, each of which will require some type of request. To check the existence of a key in the network, we need to make a request of type find_key . This request type also expects the key we're looking for as a parameter. To retrieve the value for a given key, we also use the same request type, find_key . To insert a new pair into the network, we use the request type find_and_store_key . This request type expects two parameters, the key we want to insert and the value to store for that key. If you take a look at the ask_node function's arguments, you'll notice that there are three arguments we must pass to it. Those are peer_addr , req_type and body_dict . The first one is a tuple, containing the IP address (as a string) and the port (as an integer) of the node to connect to. The second is the request type, as described above. The third is a dictionary of the parameters that must be included for that request type. Handling responses Since E-Chord is a network, whenever we make a request to one of its nodes, we expect a response. Just like the requests, E-Chord's responses follow a specific format. In general, after we make a request, we receive a response that contains a header and a body. The header contains metadata about the response, while the body contains the response data itself. Usually, this means that we check the header for the request status and the body for resulting data. In fact, every response will always contain a status field in the header. Its value is an integer code, borrowed from HTTP response status codes , that indicates the response status. The same program using E-Chord Let's now attempt to rewrite the Login System program, using E-Chord as our dictionary. We'll start by placing the code for the ask_node function in a separate file, ask_node.py , and then import it into our program, using the line from ask_node import ask_node . Since the E-Chord network is already running, we don't need to do anything when it comes to creating our dictionary. Next, let's look at the first two instances where the dictionary is used, when a user logs in. if username not in users or users[username] != password: Here, we need to check for the existence of the username. If the username exists, we need to check whether the provided password matches the one we have in storage. Remember how the request type for checking whether a key exists and for retrieving a key's value is the same? This is convenient, because we would prefer to only make one request here, not two. As such, we'll ask our node to find the provided username. The response will indicate if it exists and, if it does, will contain the password. response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) We then need to determine whether the key exists. To do this, we check the response status. It will either be 200, which means the key exists, or 404, which means it doesn't. If it does, we move to the response body, where the value field will contain the value for the key, which is the password. if response[\"header\"][\"status\"] == 404 or response[\"body\"][\"value\"] != password: print(\"Incorrect credentials!\") continue As for the register part, we need to check if a username already exists, so as not to override it. This is done exactly as above. In case the username does not exist, we need to store a pair for the new user. To do this, we use the find_and_store_key request type. response = ask_node(NODE_ADDR, \"find_and_store_key\", {\"key\": username, \"value\": password}) In theory, this request should never fail. In practice, a routing error could cause the response status to be 404. If that were to happen, we could repeat the request, possibly to a different node. For the purposes of this tutorial, however, that will not be necessary. This is it! The full version of the program, using E-Chord as its dictionary, can be seen below. from ask_node import ask_node NODE_ADDR = (\"localhost\", 9150) while True: print(\"Welcome to Login System!\") c = input(\"Would you like to login or register (l/r)? \") if c == \"l\": username = input(\"Username: \") password = input(\"Password: \") response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) if response[\"header\"][\"status\"] == 404 or response[\"body\"][\"value\"] != password: print(\"Incorrect credentials!\") continue print(f\"Welcome, {username}!\") elif c == \"r\": username = input(\"Username: \") password = input(\"Password: \") response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) if response[\"header\"][\"status\"] == 200: print(\"Username taken!\") continue response = ask_node(NODE_ADDR, \"find_and_store_key\", {\"key\": username, \"value\": password}) print(\"Successfully registered!\") Let's now try the same sample execution as with the regular version of the program. Welcome to Login System! Would you like to login or register (l/r)? r Username: Emily Password: ilovecats2 Successfully registered! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovewats2 Incorrect credentials! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovecats2 Welcome, Emily! As expected, our output is exactly the same as before! This means E-Chord has correctly stored and retrieved the pair for us, and we have communicated with it through our own software. At this point, play around with the Login System programs. Try different inputs for both, and see if the version using E-Chord works as expected. Do not forget that, since the network is running separately to your program, if you quit and start your program again, the users you've already created will still be there (since they're stored on the network). This is contrary to the original version, where restarting the program erases the dictionary data you've previously inserted. If the E-Chord network you're using is simulated on one computer, you may also run the visualizer script at the scripts directory to view the nodes in the network and see which of them store the keys you insert. Conclusion In this tutorial, you set up a simple program of your own, which communicates with an E-Chord network directly and uses it as its dictionary to store data. You made requests towards a node in the network and read its responses to achieve this. For a full list of requests and their expected responses, visit the communication reference .","title":"Communicating with an active E-Chord network through separate software"},{"location":"tutorials/communication/#communicating-with-an-active-e-chord-network-through-separate-software","text":"In this tutorial, we will create software that communicates with an active E-Chord network and uses it to store and retrieve data. We'll assume that you already have an E-Chord network up and running. This can either be a network simulation (running on one machine), or an actual E-Chord network. If you haven't done this yet, consider taking a look at the previous tutorials.","title":"Communicating with an active E-Chord network through separate software"},{"location":"tutorials/communication/#a-basic-program-using-a-hash-table","text":"Since E-Chord is a distributed hash table, we'll first write a program that uses a regular hash table. After this, we'll replace the hash table with E-Chord, and show that the functionality is the same. We'll write the program in Python, where the equivalent of a hash table is a dictionary. The program will implement a simple login system. Users will be able to register using a username and password, and login to the system. Obviously, we won't focus on any sort of security for this. We also won't implement any form of defensive programming. This will only serve to demonstrate the function of E-Chord as a distributed hash table. users = {} while True: print(\"Welcome to Login System!\") c = input(\"Would you like to login or register (l/r)? \") if c == \"l\": username = input(\"Username: \") password = input(\"Password: \") if username not in users or users[username] != password: print(\"Incorrect credentials!\") continue print(f\"Welcome, {username}!\") elif c == \"r\": username = input(\"Username: \") password = input(\"Password: \") if username in users: print(\"Username taken!\") continue users[username] = password print(\"Successfully registered!\") The above program uses a hash table to store usernames, with the value corresponding to their password. Whenever a new user registers, the hash table is checked for the existence of the username. If it doesn't exist, it is added with the provided password as a value. When a user tries to log in, the hash table is checked for the existence of the username. If it exists and the value is the same as the provided password, login is successful. Below is a sample execution of the program: Welcome to Login System! Would you like to login or register (l/r)? r Username: Emily Password: ilovecats2 Successfully registered! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovewats2 Incorrect credentials! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovecats2 Welcome, Emily!","title":"A basic program using a hash table"},{"location":"tutorials/communication/#communication-with-the-e-chord-network","text":"When communicating with an E-Chord network, we essentially need to talk to one of the nodes of the network. As such, we need to know the address of some node. If you've started the network yourself, you should know the addresses of the nodes within it, so use one of them. If you've used the scripts to create the network, open the config/params.json file. Under testing , you'll find the parameter initial_port . Its value is the port of the first node that was started by the script, incremented by 1 for each new node. As such, one possible address you can use is localhost:[initial_port] . If that one doesn't work, increment the port value by 1 and try again. Another way to find an active node in the network is simply by observing the output of the seed server. The seed server regularly polls nodes to confirm they're still there, so looking at its output, you'll find the addresses of recently polled nodes. Remember that an empty IP address string is equivalent to localhost . E-Chord nodes communicate using TCP sockets. Additionally, messages between nodes are all in JSON, and follow a specific format. For an in-depth explanation on how to structure your messages when communicating with the network, refer to this guide and the equivalent reference guide . In order to simplify this process here, we will use an existing function. This function, called ask_node , connects to the node with the address we provide and makes a request of a certain type. import socket import json def ask_node(peer_addr, req_type, body_dict, custom_timeout=10): \"\"\" Sends a request and returns the response :param peer_addr: (IP, port) of peer :param req_type: type of request for request header :param body_dict: dictionary of body :param custom_timeout: timeout for request :return: string response of peer \"\"\" request_dict = {\"header\": {\"type\": req_type}, \"body\": body_dict} request_msg = json.dumps(request_dict, indent=2) with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client: client.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) client.settimeout(custom_timeout) try: client.connect(peer_addr) client.sendall(request_msg.encode()) data = client.recv(8192).decode() except (socket.error, socket.timeout): return None if not data: return None return json.loads(data) There are various request types that can be made. As mentioned before, a full list can be viewed in the corresponding guides. As we move forward, we'll translate the dictionary (i.e hash map) operations into the appropriate requests for the E-Chord network.","title":"Communication with the E-Chord network"},{"location":"tutorials/communication/#points-of-interest-in-the-hash-table-program","text":"Let's return to the Login System program from before and identify the points where dictionary operations are used. We'll then discuss how these operations can be translated into their equivalents for E-Chord. Firstly, at the very top, we create an empty dictionary. The next time the dictionary is used is when a user tries to login. if username not in users or users[username] != password: In fact, there are two dictionary operations here. The first one is username not in users , which checks for the existence of a key in the dictionary, while the second is users[username] , which retrieves a value from the dictionary by key. Finally, we use the dictionary twice when a user registers. Firstly, like before, we check for the existence of a username: if username not in users: If a username doesn't exist, we register the user with the credentials given: users[username] = password In total, we have four points where we perform dictionary operations. Two of those consist of checking for the existence of a key, one of getting the value for a key and one of inserting a new pair.","title":"Points of interest in the hash table program"},{"location":"tutorials/communication/#equivalent-e-chord-operations","text":"To perform these operations in the E-Chord network, we'll have to make the appropriate requests to a node in the network. We have three types of operations, each of which will require some type of request. To check the existence of a key in the network, we need to make a request of type find_key . This request type also expects the key we're looking for as a parameter. To retrieve the value for a given key, we also use the same request type, find_key . To insert a new pair into the network, we use the request type find_and_store_key . This request type expects two parameters, the key we want to insert and the value to store for that key. If you take a look at the ask_node function's arguments, you'll notice that there are three arguments we must pass to it. Those are peer_addr , req_type and body_dict . The first one is a tuple, containing the IP address (as a string) and the port (as an integer) of the node to connect to. The second is the request type, as described above. The third is a dictionary of the parameters that must be included for that request type.","title":"Equivalent E-Chord operations"},{"location":"tutorials/communication/#handling-responses","text":"Since E-Chord is a network, whenever we make a request to one of its nodes, we expect a response. Just like the requests, E-Chord's responses follow a specific format. In general, after we make a request, we receive a response that contains a header and a body. The header contains metadata about the response, while the body contains the response data itself. Usually, this means that we check the header for the request status and the body for resulting data. In fact, every response will always contain a status field in the header. Its value is an integer code, borrowed from HTTP response status codes , that indicates the response status.","title":"Handling responses"},{"location":"tutorials/communication/#the-same-program-using-e-chord","text":"Let's now attempt to rewrite the Login System program, using E-Chord as our dictionary. We'll start by placing the code for the ask_node function in a separate file, ask_node.py , and then import it into our program, using the line from ask_node import ask_node . Since the E-Chord network is already running, we don't need to do anything when it comes to creating our dictionary. Next, let's look at the first two instances where the dictionary is used, when a user logs in. if username not in users or users[username] != password: Here, we need to check for the existence of the username. If the username exists, we need to check whether the provided password matches the one we have in storage. Remember how the request type for checking whether a key exists and for retrieving a key's value is the same? This is convenient, because we would prefer to only make one request here, not two. As such, we'll ask our node to find the provided username. The response will indicate if it exists and, if it does, will contain the password. response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) We then need to determine whether the key exists. To do this, we check the response status. It will either be 200, which means the key exists, or 404, which means it doesn't. If it does, we move to the response body, where the value field will contain the value for the key, which is the password. if response[\"header\"][\"status\"] == 404 or response[\"body\"][\"value\"] != password: print(\"Incorrect credentials!\") continue As for the register part, we need to check if a username already exists, so as not to override it. This is done exactly as above. In case the username does not exist, we need to store a pair for the new user. To do this, we use the find_and_store_key request type. response = ask_node(NODE_ADDR, \"find_and_store_key\", {\"key\": username, \"value\": password}) In theory, this request should never fail. In practice, a routing error could cause the response status to be 404. If that were to happen, we could repeat the request, possibly to a different node. For the purposes of this tutorial, however, that will not be necessary. This is it! The full version of the program, using E-Chord as its dictionary, can be seen below. from ask_node import ask_node NODE_ADDR = (\"localhost\", 9150) while True: print(\"Welcome to Login System!\") c = input(\"Would you like to login or register (l/r)? \") if c == \"l\": username = input(\"Username: \") password = input(\"Password: \") response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) if response[\"header\"][\"status\"] == 404 or response[\"body\"][\"value\"] != password: print(\"Incorrect credentials!\") continue print(f\"Welcome, {username}!\") elif c == \"r\": username = input(\"Username: \") password = input(\"Password: \") response = ask_node(NODE_ADDR, \"find_key\", {\"key\": username}) if response[\"header\"][\"status\"] == 200: print(\"Username taken!\") continue response = ask_node(NODE_ADDR, \"find_and_store_key\", {\"key\": username, \"value\": password}) print(\"Successfully registered!\") Let's now try the same sample execution as with the regular version of the program. Welcome to Login System! Would you like to login or register (l/r)? r Username: Emily Password: ilovecats2 Successfully registered! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovewats2 Incorrect credentials! Welcome to Login System! Would you like to login or register (l/r)? l Username: Emily Password: ilovecats2 Welcome, Emily! As expected, our output is exactly the same as before! This means E-Chord has correctly stored and retrieved the pair for us, and we have communicated with it through our own software. At this point, play around with the Login System programs. Try different inputs for both, and see if the version using E-Chord works as expected. Do not forget that, since the network is running separately to your program, if you quit and start your program again, the users you've already created will still be there (since they're stored on the network). This is contrary to the original version, where restarting the program erases the dictionary data you've previously inserted. If the E-Chord network you're using is simulated on one computer, you may also run the visualizer script at the scripts directory to view the nodes in the network and see which of them store the keys you insert.","title":"The same program using E-Chord"},{"location":"tutorials/communication/#conclusion","text":"In this tutorial, you set up a simple program of your own, which communicates with an E-Chord network directly and uses it as its dictionary to store data. You made requests towards a node in the network and read its responses to achieve this. For a full list of requests and their expected responses, visit the communication reference .","title":"Conclusion"},{"location":"tutorials/mass-simulation/","text":"Using provided scripts to simulate a large E-Chord network on a single computer The purpose of this tutorial is to use scripts provided with E-Chord to set up a large E-Chord network. As in previous tutorials, the network will run on a single computer. As such, we refer to it as a simulation, rather than a real network. To follow along, you'll need to get both the seed server and the main E-Chord repositories. For more information on how to do this, refer to the setting up guide . Starting the seed server As usual, the first step to setting up an E-Chord network is executing the seed server. If you've done this before, feel free to repeat that process and skip this section. Open a terminal at the location of the seed server and execute the command python3 server.py . You should see output similar to the following: MainThread-INFO: Loaded params MainThread-INFO: Starting node on 192.168.1.12:8000 If this isn't the case, the most common issue will be the unavailability of the port used. Open the params.json file, located in the same directory, and change the port under host to a different number. Try running the server again, until you you get output like the above. Take note of the IP address and port number of the seed server, as seen in the output. You'll need this information for the next part. Configuring node parameters Before adding any nodes to the network, we must make sure we've set up the execution parameters as required. Those can be found in the main E-Chord repository, at config/params.json . Open that file using a text editor of your choice. The contents should look something like this: { \"seed_server\": { \"ip\": \"192.168.1.12\", \"port\": 8000, \"attempt_limit\": 4 }, \"host\": { \"server_port\": 9150 }, \"ring\": { \"bits\": 14, \"fallback_fingers\": 2, \"stabilize_delay\": 3 }, \"net\": { \"timeout\": 10, \"connection_lifespan\": 60, \"data_size\": 8192 }, \"logging\": { \"level\": \"INFO\" }, \"testing\": { \"initial_port\": 9150, \"total_nodes\": 100, \"stabilize_wait_period\": 60, \"percentage_to_remove\": 50 } } If these look overwhelming, don't worry. We'll only use a few here, so you can ignore most of these. For a complete list of parameters and their function, refer to the corresponding reference guide . The first parameters we'll need to change can be seen first, under seed_server . We'll need to change the values for ip and port to the values we got when we started the seed server. The commonly used format for these is [ip address]:[port] , so replace the default IP address and port with the ones your seed server is using. For instance, our seed server output was 192.168.1.12:8000 . Therefore, we set the IP address to 192.168.1.12, and the port to 8000, as seen above. The next set of parameters we'll concern ourselves with are those under testing . These are parameters that describe the function of the simulation scripts. Specifically, we'll change the value of the total_nodes parameter to 40. This parameter describes the number of nodes that will actually be created in our simulation. In other words, the number of simulated computers in our network. Feel free to use a different value if you want to! Just make sure it isn't too large, since it will affect your waiting time later on. Once you've saved your changes, you're ready to move on to the next part. Executing the visualizer Next up, make sure to execute the visualizer script. Open a terminal window at the scripts directory, located in the main E-Chord directory, and execute python3 visualizer.py . What you should see is an empty table. This is expected, since we haven't yet added any nodes to the network. Adding nodes to the network Now that our parameters are configured, we're ready to execute our first script. If you've followed the first tutorial, you might have noticed that adding each node to the network individually can be time-consuming. To avoid this, we can instead use a script to add a number of nodes, as determined by the parameter we set. Open a new terminal window at the scripts directory and execute the command python3 mass_node_join.py . You'll start getting a large amount of output. You can ignore that and switch back to the visualizer window. Once you do so, you'll see that every time the script updates, more nodes are added to the network. Pay attention to the IDs for each node's successor. You might see that for a lot of them, the successor isn't the same as the ID of the node below it. This is because the network hasn't yet stabilized, as indicated by the stability percentage at the bottom. Wait and observe the visualizer. With time, you'll notice the stability percentage going up. If you look at individual nodes in the table, you'll find that more and more of them start pointing to the correct successor, the node under them in the table. Eventually, the network stability percentage should reach 100%. Once it does, you're ready to move on to the next step (in fact, you can even move on if your stability percentage is high, but not yet 100%). Inserting data For this part, we'll need some additional data, in JSON format. Specifically, we'll want a JSON file that consists of a JSON Object, containing multiple keys. If you already have a file with that format, you may use that. If not, you can use this randomly generated sample file . Open a new terminal window at the scripts directory and execute the command python3 mass_data.py [filedir] i . Replace [filedir] with the directory of the data file you're using (relative or absolute). If the script executes correctly, you will see the script slowly inserting data into the network. You may switch over to the visualizer as well, where you'll notice the total number of keys increasing, as well as various nodes starting to hold those keys. The mass_data script will also notify you of the failure percentage for the insertions. This is generally expected to be 0, but do not panic if it isn't! E-Chord is a distributed system and, as such, routing failures will always happen. In this case, the script will ignore keys that fail to be inserted, but in a real scenario, we would attempt to insert them again until we succeed. Once the insertion script finishes, observe the visualizer window. Below is a sample of what part of the table should look like. \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 493 -> 665 \u2551 16263 <- 15162 \u2551 493: 380 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 665 -> 1558 \u2551 15162 <- 15000 \u2551 665: 106 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 1558 -> 2178 \u2551 15000 <- 14250 \u2551 1558: 604 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2178 -> 2358 \u2551 14250 <- 13662 \u2551 2178: 350 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2358 -> 2449 \u2551 13662 <- 13637 \u2551 2358: 115 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 You'll find that the keys have been spread to multiple nodes, instead of all of them being saved in one. Pay attention to how the keys are distributed. On average, it will probably look like each node stores a similar number of keys, with the occasional extremes. A more careful look might, however, reveal that the number of keys stored in each node is somehow proportional to the distance between the node's ID and the previous node ID. The reason for this relates to the Chord protocol itself. For further analysis, visit the Chord protocol discussion . At the bottom of the script, you'll get a sum of the total keys saved in the network: Total keys: 9808 Stability: 100.00% Because of routing failures, we were only able to save 9808 out of the 10000 keys we had. This might be different in your case. Looking up inserted data Now that we've inserted our data, it is time to figure out whether we can actually retrieve what we stored. To do this, we use the same script as before, only with l instead of i as a final argument. Specifically, the command we'll execute (at the scripts directory) is python3 mass_data.py [filedir] l . Once the script is correctly executed, the output should look similar to the one of the insertion variant. This time, the failure percentage we see indicates lookup failures. You'll observe that that percentage is likely higher than the insertion percentage from before. In fact, the lowest possible failure percentage you can expect here is the insertion failure percentage. This is because, since we're using the entire file again, we're looking up every key, including those that weren't inserted. Still, if everything goes well, you should see a relatively low failure percentage. If you're instead getting very high failure percentages, it helps to restart your computer and try setting up the network again. Removing half the network's nodes A quality that is generally desired of any distributed system is the ability for fault tolerance. Distributed systems, by their nature, are always prone to various types of faults. One of those faults is the sudden loss of network nodes (because a computer went offline, or lost power, etc). To test E-Chord's ability for tolerance to such faults, we're going to intentionally drop half the network's nodes. We'll then let the system stabilize and try to look up our data again. To start this process, go to the scripts directory and execute the python3 mass_node_leave.py command. This will remove half the nodes from the network. If you return to the visualizer, you'll notice that some keys have been lost, and stability has dropped. At this point, wait and observe the visualizer again. It will likely take a while, but eventually, stability should return to 100%. Nevertheless, once stability is high enough, we can move on to the next step. Looking up data after simulated failure Now that we've intentionally caused a mass node failure, and after the network has been given time to stabilize, we can look up our data again. At the scripts directory, execute the mass_data script again, in the same way we did before when we initially looked up the data. You should notice that, even though the failure percentage will generally be higher, the network has more or less been able to come back from losing even as high a number of nodes as half its nodes at once. In fact, most of the failures will occur due to missing data, either because insertion itself failed, or because data might have been lost due to the node failures. Conclusion In this tutorial, you set up a simulation of a large E-Chord network on a single computer. Using the scripts, you managed to insert data into the network, and subsequently retrieve that data. Afterwards, you caused a massive node failure and observed how the network re-stabilizes over time. Finally, you looked up your data again.","title":"Using provided scripts to simulate a large E-Chord network on a single computer"},{"location":"tutorials/mass-simulation/#using-provided-scripts-to-simulate-a-large-e-chord-network-on-a-single-computer","text":"The purpose of this tutorial is to use scripts provided with E-Chord to set up a large E-Chord network. As in previous tutorials, the network will run on a single computer. As such, we refer to it as a simulation, rather than a real network. To follow along, you'll need to get both the seed server and the main E-Chord repositories. For more information on how to do this, refer to the setting up guide .","title":"Using provided scripts to simulate a large E-Chord network on a single computer"},{"location":"tutorials/mass-simulation/#starting-the-seed-server","text":"As usual, the first step to setting up an E-Chord network is executing the seed server. If you've done this before, feel free to repeat that process and skip this section. Open a terminal at the location of the seed server and execute the command python3 server.py . You should see output similar to the following: MainThread-INFO: Loaded params MainThread-INFO: Starting node on 192.168.1.12:8000 If this isn't the case, the most common issue will be the unavailability of the port used. Open the params.json file, located in the same directory, and change the port under host to a different number. Try running the server again, until you you get output like the above. Take note of the IP address and port number of the seed server, as seen in the output. You'll need this information for the next part.","title":"Starting the seed server"},{"location":"tutorials/mass-simulation/#configuring-node-parameters","text":"Before adding any nodes to the network, we must make sure we've set up the execution parameters as required. Those can be found in the main E-Chord repository, at config/params.json . Open that file using a text editor of your choice. The contents should look something like this: { \"seed_server\": { \"ip\": \"192.168.1.12\", \"port\": 8000, \"attempt_limit\": 4 }, \"host\": { \"server_port\": 9150 }, \"ring\": { \"bits\": 14, \"fallback_fingers\": 2, \"stabilize_delay\": 3 }, \"net\": { \"timeout\": 10, \"connection_lifespan\": 60, \"data_size\": 8192 }, \"logging\": { \"level\": \"INFO\" }, \"testing\": { \"initial_port\": 9150, \"total_nodes\": 100, \"stabilize_wait_period\": 60, \"percentage_to_remove\": 50 } } If these look overwhelming, don't worry. We'll only use a few here, so you can ignore most of these. For a complete list of parameters and their function, refer to the corresponding reference guide . The first parameters we'll need to change can be seen first, under seed_server . We'll need to change the values for ip and port to the values we got when we started the seed server. The commonly used format for these is [ip address]:[port] , so replace the default IP address and port with the ones your seed server is using. For instance, our seed server output was 192.168.1.12:8000 . Therefore, we set the IP address to 192.168.1.12, and the port to 8000, as seen above. The next set of parameters we'll concern ourselves with are those under testing . These are parameters that describe the function of the simulation scripts. Specifically, we'll change the value of the total_nodes parameter to 40. This parameter describes the number of nodes that will actually be created in our simulation. In other words, the number of simulated computers in our network. Feel free to use a different value if you want to! Just make sure it isn't too large, since it will affect your waiting time later on. Once you've saved your changes, you're ready to move on to the next part.","title":"Configuring node parameters"},{"location":"tutorials/mass-simulation/#executing-the-visualizer","text":"Next up, make sure to execute the visualizer script. Open a terminal window at the scripts directory, located in the main E-Chord directory, and execute python3 visualizer.py . What you should see is an empty table. This is expected, since we haven't yet added any nodes to the network.","title":"Executing the visualizer"},{"location":"tutorials/mass-simulation/#adding-nodes-to-the-network","text":"Now that our parameters are configured, we're ready to execute our first script. If you've followed the first tutorial, you might have noticed that adding each node to the network individually can be time-consuming. To avoid this, we can instead use a script to add a number of nodes, as determined by the parameter we set. Open a new terminal window at the scripts directory and execute the command python3 mass_node_join.py . You'll start getting a large amount of output. You can ignore that and switch back to the visualizer window. Once you do so, you'll see that every time the script updates, more nodes are added to the network. Pay attention to the IDs for each node's successor. You might see that for a lot of them, the successor isn't the same as the ID of the node below it. This is because the network hasn't yet stabilized, as indicated by the stability percentage at the bottom. Wait and observe the visualizer. With time, you'll notice the stability percentage going up. If you look at individual nodes in the table, you'll find that more and more of them start pointing to the correct successor, the node under them in the table. Eventually, the network stability percentage should reach 100%. Once it does, you're ready to move on to the next step (in fact, you can even move on if your stability percentage is high, but not yet 100%).","title":"Adding nodes to the network"},{"location":"tutorials/mass-simulation/#inserting-data","text":"For this part, we'll need some additional data, in JSON format. Specifically, we'll want a JSON file that consists of a JSON Object, containing multiple keys. If you already have a file with that format, you may use that. If not, you can use this randomly generated sample file . Open a new terminal window at the scripts directory and execute the command python3 mass_data.py [filedir] i . Replace [filedir] with the directory of the data file you're using (relative or absolute). If the script executes correctly, you will see the script slowly inserting data into the network. You may switch over to the visualizer as well, where you'll notice the total number of keys increasing, as well as various nodes starting to hold those keys. The mass_data script will also notify you of the failure percentage for the insertions. This is generally expected to be 0, but do not panic if it isn't! E-Chord is a distributed system and, as such, routing failures will always happen. In this case, the script will ignore keys that fail to be inserted, but in a real scenario, we would attempt to insert them again until we succeed. Once the insertion script finishes, observe the visualizer window. Below is a sample of what part of the table should look like. \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Successors \u2551 Predecessors \u2551 Keys \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 493 -> 665 \u2551 16263 <- 15162 \u2551 493: 380 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 665 -> 1558 \u2551 15162 <- 15000 \u2551 665: 106 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 1558 -> 2178 \u2551 15000 <- 14250 \u2551 1558: 604 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2178 -> 2358 \u2551 14250 <- 13662 \u2551 2178: 350 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 2358 -> 2449 \u2551 13662 <- 13637 \u2551 2358: 115 \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 You'll find that the keys have been spread to multiple nodes, instead of all of them being saved in one. Pay attention to how the keys are distributed. On average, it will probably look like each node stores a similar number of keys, with the occasional extremes. A more careful look might, however, reveal that the number of keys stored in each node is somehow proportional to the distance between the node's ID and the previous node ID. The reason for this relates to the Chord protocol itself. For further analysis, visit the Chord protocol discussion . At the bottom of the script, you'll get a sum of the total keys saved in the network: Total keys: 9808 Stability: 100.00% Because of routing failures, we were only able to save 9808 out of the 10000 keys we had. This might be different in your case.","title":"Inserting data"},{"location":"tutorials/mass-simulation/#looking-up-inserted-data","text":"Now that we've inserted our data, it is time to figure out whether we can actually retrieve what we stored. To do this, we use the same script as before, only with l instead of i as a final argument. Specifically, the command we'll execute (at the scripts directory) is python3 mass_data.py [filedir] l . Once the script is correctly executed, the output should look similar to the one of the insertion variant. This time, the failure percentage we see indicates lookup failures. You'll observe that that percentage is likely higher than the insertion percentage from before. In fact, the lowest possible failure percentage you can expect here is the insertion failure percentage. This is because, since we're using the entire file again, we're looking up every key, including those that weren't inserted. Still, if everything goes well, you should see a relatively low failure percentage. If you're instead getting very high failure percentages, it helps to restart your computer and try setting up the network again.","title":"Looking up inserted data"},{"location":"tutorials/mass-simulation/#removing-half-the-networks-nodes","text":"A quality that is generally desired of any distributed system is the ability for fault tolerance. Distributed systems, by their nature, are always prone to various types of faults. One of those faults is the sudden loss of network nodes (because a computer went offline, or lost power, etc). To test E-Chord's ability for tolerance to such faults, we're going to intentionally drop half the network's nodes. We'll then let the system stabilize and try to look up our data again. To start this process, go to the scripts directory and execute the python3 mass_node_leave.py command. This will remove half the nodes from the network. If you return to the visualizer, you'll notice that some keys have been lost, and stability has dropped. At this point, wait and observe the visualizer again. It will likely take a while, but eventually, stability should return to 100%. Nevertheless, once stability is high enough, we can move on to the next step.","title":"Removing half the network's nodes"},{"location":"tutorials/mass-simulation/#looking-up-data-after-simulated-failure","text":"Now that we've intentionally caused a mass node failure, and after the network has been given time to stabilize, we can look up our data again. At the scripts directory, execute the mass_data script again, in the same way we did before when we initially looked up the data. You should notice that, even though the failure percentage will generally be higher, the network has more or less been able to come back from losing even as high a number of nodes as half its nodes at once. In fact, most of the failures will occur due to missing data, either because insertion itself failed, or because data might have been lost due to the node failures.","title":"Looking up data after simulated failure"},{"location":"tutorials/mass-simulation/#conclusion","text":"In this tutorial, you set up a simulation of a large E-Chord network on a single computer. Using the scripts, you managed to insert data into the network, and subsequently retrieve that data. Afterwards, you caused a massive node failure and observed how the network re-stabilizes over time. Finally, you looked up your data again.","title":"Conclusion"}]}